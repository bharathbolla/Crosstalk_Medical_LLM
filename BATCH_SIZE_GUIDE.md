# Batch Size Optimization Guide

## TL;DR

**For T4 16GB with BioBERT/BERT-base**:
- âœ… **Batch size 32** (optimal, ~1.5x faster than 16)
- âœ… Batch size 24 (safe fallback)
- âš ï¸ Batch size 16 (current default - suboptimal!)

**Run this first**:
```bash
python find_optimal_batch_size.py --model dmis-lab/biobert-v1.1
```

---

## Why Batch Size Matters

### Performance Impact:

| Batch Size | VRAM Used | Training Time | Samples/sec | Efficiency |
|------------|-----------|---------------|-------------|------------|
| 8 | 5GB (31%) | 4.0 hours | 22 | âŒ Slow |
| 16 | 8GB (50%) | 2.5 hours | 45 | âš ï¸ OK |
| **24** | 11GB (69%) | **1.8 hours** | **65** | âœ… **Good** |
| **32** | 14GB (88%) | **1.5 hours** | **80** | âœ… **Best** |

**Using batch size 32 vs 16**:
- âš¡ **40% faster** training
- ðŸ’° **40% less GPU cost** (if paid)
- ðŸ“Š **Same final performance** (with same learning rate tuning)

---

## Finding Your Optimal Batch Size

### Method 1: Automated Script (Recommended)

```bash
# On Kaggle, add this cell BEFORE your training:

!python find_optimal_batch_size.py --model dmis-lab/biobert-v1.1 --max_length 512

# Output will show:
# âœ… OPTIMAL BATCH SIZE: 32
# Update your config:
#   CONFIG["batch_size"] = 32
```

### Method 2: Manual Testing

Start with batch size 16, then increase:

```python
# Cell in notebook
batch_sizes_to_test = [16, 24, 32, 40]

for bs in batch_sizes_to_test:
    print(f"\nTesting batch size {bs}...")
    try:
        # Quick 10-step test
        CONFIG['batch_size'] = bs
        CONFIG['max_samples_per_dataset'] = 100
        # ... run training for 10 steps ...
        print(f"âœ… Batch size {bs} works!")
    except RuntimeError as e:
        if "out of memory" in str(e):
            print(f"âŒ Batch size {bs} OOM - use {last_working_bs}")
            break
```

---

## Batch Size by Model & GPU

### T4 16GB (Kaggle Free Tier)

| Model | Params | Max Length | Optimal Batch | Max Batch (risky) |
|-------|--------|------------|---------------|-------------------|
| BERT-base | 110M | 512 | **32** | 40 |
| BioBERT v1.1 | 110M | 512 | **32** | 40 |
| PubMedBERT | 110M | 512 | **32** | 40 |
| SciBERT | 110M | 512 | **32** | 40 |
| BERT-base | 110M | 256 | **64** | 80 |

### A100 40GB (If Available)

| Model | Params | Max Length | Optimal Batch |
|-------|--------|------------|---------------|
| BERT-base | 110M | 512 | **128** |
| Llama-3.2-3B | 3B | 512 | **32** |
| Llama-3.2-3B (4-bit) | 3B | 512 | **64** |

---

## Important Considerations

### 1. **Effective Batch Size** (What Actually Matters)

```python
# These are EQUIVALENT in terms of gradient updates:

# Option A: Batch size 32, no accumulation
CONFIG['batch_size'] = 32
CONFIG['gradient_accumulation_steps'] = 1
# Effective batch size: 32

# Option B: Batch size 16, accumulate 2 steps
CONFIG['batch_size'] = 16
CONFIG['gradient_accumulation_steps'] = 2
# Effective batch size: 32 (same!)

# Option C: Batch size 8, accumulate 4 steps
CONFIG['batch_size'] = 8
CONFIG['gradient_accumulation_steps'] = 4
# Effective batch size: 32 (same!)
```

**Why use gradient accumulation?**
- If OOM with batch 32
- Train larger models
- Simulate larger batches on small GPU

**Downside**: Slower (more forward/backward passes)

### 2. **Learning Rate Scaling**

**Rule of thumb**: If you change batch size, adjust learning rate proportionally.

```python
# Original
batch_size = 16
learning_rate = 2e-5

# Doubled batch size â†’ increase LR slightly
batch_size = 32
learning_rate = 3e-5  # Not 4e-5! Scale by sqrt(2) â‰ˆ 1.4

# Halved batch size â†’ decrease LR
batch_size = 8
learning_rate = 1.5e-5
```

**Or use warmup** to be safe:
```python
CONFIG['warmup_steps'] = 500  # Gradual LR increase
CONFIG['learning_rate'] = 2e-5  # Same LR works with warmup
```

### 3. **Multi-Task Training**

For multi-task, use **smaller batch size** per task:

```python
# Single-task
CONFIG['batch_size'] = 32

# Multi-task (2 datasets)
CONFIG['batch_size'] = 16  # Half size
# Reason: Sampling from multiple datasets needs more memory

# Multi-task (8 datasets)
CONFIG['batch_size'] = 8  # Quarter size
```

---

## Practical Recommendations

### For Your Experiments:

**Single-Task Baselines (S1)**:
```python
CONFIG = {
    'batch_size': 32,  # Use optimal!
    'learning_rate': 2e-5,
    'warmup_steps': 500,
}
```

**Multi-Task (S2)**:
```python
CONFIG = {
    'batch_size': 16,  # Half of single-task
    'learning_rate': 2e-5,
    'warmup_steps': 500,
}
```

**Quick Tests**:
```python
CONFIG = {
    'batch_size': 8,  # Small for safety
    'max_samples_per_dataset': 100,
    'num_epochs': 1,
}
```

---

## Common Issues

### OOM (Out of Memory)

**Symptoms**:
```
RuntimeError: CUDA out of memory. Tried to allocate X.XX GiB
```

**Solutions** (in order of preference):

1. **Reduce batch size**:
   ```python
   CONFIG['batch_size'] = 16  # From 32
   ```

2. **Reduce sequence length**:
   ```python
   CONFIG['max_length'] = 256  # From 512
   # Medical text is often short anyway!
   ```

3. **Use gradient accumulation**:
   ```python
   CONFIG['batch_size'] = 16
   CONFIG['gradient_accumulation_steps'] = 2
   # Effective batch size still 32
   ```

4. **Clear cache between runs**:
   ```python
   import torch
   torch.cuda.empty_cache()
   ```

### Slow Training

**Symptoms**: Training taking >3 hours for small dataset

**Solutions**:

1. **Increase batch size** (if VRAM allows):
   ```python
   # Test with find_optimal_batch_size.py
   CONFIG['batch_size'] = 32  # From 16
   ```

2. **Use FP16** (already enabled in our notebook):
   ```python
   # In TrainingArguments
   fp16=torch.cuda.is_available()  # Already set!
   ```

3. **Reduce eval frequency**:
   ```python
   CONFIG['eval_steps'] = 500  # From 250
   # Evaluate less often = faster
   ```

### VRAM Not Being Utilized

**Symptoms**: GPU showing only 8GB/16GB used

**Problem**: Batch size too small!

**Solution**:
```python
# Run find_optimal_batch_size.py
# Update CONFIG['batch_size'] to recommended value
```

---

## Advanced: Profiling Your Training

### Monitor VRAM Usage

```python
# Add to training cell
import torch

def print_gpu_memory():
    allocated = torch.cuda.memory_allocated() / 1e9
    reserved = torch.cuda.memory_reserved() / 1e9
    total = torch.cuda.get_device_properties(0).total_memory / 1e9

    print(f"GPU Memory:")
    print(f"  Allocated: {allocated:.2f}GB / {total:.2f}GB ({100*allocated/total:.1f}%)")
    print(f"  Reserved: {reserved:.2f}GB / {total:.2f}GB ({100*reserved/total:.1f}%)")

# Call after model is loaded
print_gpu_memory()
```

**Target**: 80-90% utilization (optimal)
- <70%: Increase batch size
- >95%: Risk of OOM, decrease batch size

### Benchmark Different Batch Sizes

```python
import time

batch_sizes = [16, 24, 32]
results = []

for bs in batch_sizes:
    CONFIG['batch_size'] = bs
    CONFIG['max_samples_per_dataset'] = 500
    CONFIG['num_epochs'] = 1

    start = time.time()
    trainer.train()
    elapsed = time.time() - start

    samples_per_sec = 500 / elapsed

    results.append({
        'batch_size': bs,
        'time': elapsed,
        'samples_per_sec': samples_per_sec,
    })

# Print comparison
import pandas as pd
df = pd.DataFrame(results)
print(df)
```

---

## Summary

### Quick Recommendations:

| Your Situation | Recommended Batch Size |
|----------------|------------------------|
| **Single-task, T4 16GB** | **32** |
| **Multi-task (2-3 datasets)** | **16** |
| **Multi-task (all 8)** | **8** |
| **Quick test** | **8** |
| **Got OOM** | **Reduce by half** |
| **Training too slow** | **Double it (test first)** |

### Action Items:

1. âœ… **Run**: `python find_optimal_batch_size.py`
2. âœ… **Update** `CONFIG['batch_size']` in notebook Cell 3
3. âœ… **Test** with quick run (100 samples, 1 epoch)
4. âœ… **Monitor** GPU usage during training
5. âœ… **Adjust** if needed

### Expected Results:

**Before** (batch size 16):
- Training BC2GM: ~2.5 hours
- GPU utilization: ~50%

**After** (batch size 32):
- Training BC2GM: ~1.5 hours âš¡ **40% faster!**
- GPU utilization: ~85% âœ…

---

**Pro tip**: Always run `find_optimal_batch_size.py` when trying a new model or GPU!
