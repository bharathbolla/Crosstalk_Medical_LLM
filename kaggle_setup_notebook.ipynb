{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Cross-Task Knowledge Transfer - Kaggle Setup\n",
    "\n",
    "**Project**: Medical NLP with Small Language Models  \n",
    "**Goal**: Study cross-task knowledge transfer in medical NLP tasks  \n",
    "**GPU**: T4 (16GB VRAM)  \n",
    "**Datasets**: 7 active (ChemProt disabled due to loading issues)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Checklist\n",
    "\n",
    "Before running this notebook:\n",
    "1. ‚úÖ Enable **GPU T4 x2** in Settings ‚Üí Accelerator\n",
    "2. ‚úÖ Enable **Internet** in Settings ‚Üí Internet\n",
    "3. ‚úÖ Set **Persistence** to \"Files only\" in Settings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your GitHub repository\n",
    "!git clone https://github.com/bharathbolla/Crosstalk_Medical_LLM.git\n",
    "%cd Crosstalk_Medical_LLM\n",
    "\n",
    "# Verify structure\n",
    "print(\"\\nüìÅ Repository structure:\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n# IMPORTANT: Compatible versions for bigbio datasets\n!pip install -q transformers evaluate wandb accelerate scikit-learn pyyaml\n!pip install -q pyarrow==12.0.1 datasets==2.14.0\n\nprint(\"‚úÖ Dependencies installed!\")\nprint(\"   Note: Using datasets==2.14.0 + pyarrow==12.0.1 for bigbio compatibility\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available! Check Settings ‚Üí Accelerator ‚Üí GPU T4 x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4Ô∏è‚É£ Download Datasets (15 minutes)\n\nDownloads 7 medical NLP datasets from bigbio collection.  \n**Method**: Direct load_dataset() with specific repo and config names.  \n**Note**: ChemProt excluded due to loading issues."
  },
  {
   "cell_type": "markdown",
   "source": "## 3.5Ô∏è‚É£ Fix Dataset Library (CRITICAL!)\n\n‚ö†Ô∏è **Run this cell FIRST if you get \"Dataset scripts are no longer supported\" errors**\n\nThe bigbio datasets use custom loading scripts that are blocked in newer versions of the datasets library. We need to downgrade to version 2.14.0 which still supports them.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# CRITICAL FIX: Downgrade both datasets AND pyarrow for compatibility\nprint(\"üîß Fixing dataset library compatibility...\")\nprint(\"   Installing: pyarrow==12.0.1 + datasets==2.14.0\")\nprint()\n\n!pip install -q pyarrow==12.0.1 datasets==2.14.0\n\nprint(\"\\n‚úÖ Fix applied! Libraries are now compatible.\")\nprint(\"   You can now run the download cell below.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\nfrom pathlib import Path\nimport subprocess\nimport sys\n\n# Create data directory\ndata_path = Path(\"data/raw\")\ndata_path.mkdir(parents=True, exist_ok=True)\n\nprint(\"üì• Downloading 7 medical NLP datasets from bigbio collection...\\n\")\nprint(\"=\" * 60)\n\n# IMPORTANT: Newer datasets library versions require this\nprint(\"‚öôÔ∏è  Setting up environment for bigbio datasets...\")\nprint(\"   (These datasets use custom loading scripts)\")\nprint()\n\n# Dataset configurations\ndatasets_config = {\n    \"bc2gm\": {\n        \"repo\": \"bigbio/blurb\",\n        \"config\": \"bc2gm\",\n        \"description\": \"Gene/protein NER from PubMed abstracts\"\n    },\n    \"jnlpba\": {\n        \"repo\": \"bigbio/blurb\",\n        \"config\": \"jnlpba\",\n        \"description\": \"Bio-entity NER (protein, DNA, RNA, cell line, cell type)\"\n    },\n    \"ddi\": {\n        \"repo\": \"bigbio/ddi_corpus\",\n        \"config\": \"ddi_corpus_source\",\n        \"description\": \"Drug-drug interaction extraction\"\n    },\n    \"gad\": {\n        \"repo\": \"bigbio/gad\",\n        \"config\": \"gad_blurb_bigbio_text\",\n        \"description\": \"Gene-disease association classification\"\n    },\n    \"hoc\": {\n        \"repo\": \"bigbio/hallmarks_of_cancer\",\n        \"config\": \"hallmarks_of_cancer_source\",\n        \"description\": \"Cancer hallmarks classification (multi-label)\"\n    },\n    \"pubmedqa\": {\n        \"repo\": \"bigbio/pubmed_qa\",\n        \"config\": \"pubmed_qa_labeled_fold0_source\",\n        \"description\": \"Medical question answering\"\n    },\n    \"biosses\": {\n        \"repo\": \"bigbio/biosses\",\n        \"config\": \"biosses_bigbio_pairs\",\n        \"description\": \"Biomedical sentence similarity\"\n    }\n}\n\ntotal_samples = 0\nsuccessful = 0\nfailed = []\n\nfor name, config in datasets_config.items():\n    print(f\"\\nüì¶ {name.upper()}\")\n    print(f\"   {config['description']}\")\n\n    try:\n        # CRITICAL FIX: Must use trust_remote_code=True for bigbio datasets\n        # These datasets have custom loading scripts that are safe but need explicit permission\n        dataset = load_dataset(\n            config[\"repo\"],\n            name=config[\"config\"],\n            trust_remote_code=True  # ‚ö†Ô∏è CHANGED from False to True - this is required!\n        )\n\n        # Save to disk\n        dataset.save_to_disk(str(data_path / name))\n\n        # Show stats\n        train_size = len(dataset[\"train\"])\n        total_samples += train_size\n        successful += 1\n\n        # Show split info\n        splits_info = \" + \".join([f\"{split}: {len(dataset[split])}\" for split in dataset.keys()])\n        print(f\"   ‚úì Downloaded! Splits: {splits_info}\")\n\n    except Exception as e:\n        error_msg = str(e)\n        failed.append(name)\n        \n        # Check if it's the \"dataset scripts no longer supported\" error\n        if \"Dataset scripts are no longer supported\" in error_msg:\n            print(f\"   ‚úó ERROR: Dataset scripts blocked by datasets library\")\n            print(f\"   üí° FIX: Need to downgrade datasets library\")\n            print(f\"          Run: pip install datasets==2.14.0\")\n        else:\n            print(f\"   ‚úó ERROR: {error_msg[:100]}\")\n        continue\n\n# Summary\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"‚úÖ Successfully downloaded: {successful}/7 datasets\")\nprint(f\"üìä Total training samples: {total_samples:,}\")\n\nif successful == 7:\n    print(\"\\nüéâ All datasets downloaded successfully!\")\n    print(f\"\\nDatasets saved in: {data_path.absolute()}\")\nelif successful == 0:\n    print(\"\\n‚ö†Ô∏è  FALLBACK NEEDED: All downloads failed!\")\n    print(\"\\nüîß FIX: Run this command in a cell BEFORE this one:\")\n    print(\"   !pip install -q datasets==2.14.0\")\n    print(\"\\nThen re-run this cell. The older datasets version supports custom scripts.\")\nelse:\n    print(f\"\\n‚ö†Ô∏è  Partial success - Failed datasets: {', '.join(failed)}\")\n    \nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Test Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that parsers work\n",
    "import sys\n",
    "sys.path.insert(0, \"src\")\n",
    "\n",
    "from data import TaskRegistry, BC2GMDataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Check registered tasks (should be 7 without ChemProt)\n",
    "print(f\"Registered tasks: {TaskRegistry.list_tasks()}\")\n",
    "\n",
    "# Load one dataset\n",
    "dataset = BC2GMDataset(\n",
    "    data_path=Path(\"data/raw\"),\n",
    "    split=\"train\"\n",
    ")\n",
    "print(f\"\\nLoaded {len(dataset)} BC2GM samples\")\n",
    "print(f\"First sample:\\n  {dataset[0].input_text[:150]}...\")\n",
    "\n",
    "# Check label schema\n",
    "schema = dataset.get_label_schema()\n",
    "print(f\"\\nLabel schema ({len(schema)} labels): {list(schema.keys())}\")\n",
    "\n",
    "print(\"\\n‚úÖ Everything works! Ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Smoke Test - Quick Training Test (10 minutes)\n",
    "\n",
    "Train BERT on 100 samples for 50 steps to verify the pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from src.data import BC2GMDataset\n",
    "from src.data.collators import NERCollator\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üöÄ Starting smoke test...\\n\")\n",
    "\n",
    "# 1. Load tiny subset (100 samples only)\n",
    "dataset = BC2GMDataset(data_path=Path(\"data/raw\"), split=\"train\")\n",
    "small_dataset = [dataset[i] for i in range(100)]\n",
    "print(f\"‚úì Loaded {len(small_dataset)} samples\")\n",
    "\n",
    "# 2. Load BERT model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"‚úì Loaded tokenizer: {model_name}\")\n",
    "\n",
    "label_schema = dataset.get_label_schema()\n",
    "num_labels = len(label_schema)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ").to(\"cuda\")\n",
    "print(f\"‚úì Loaded model: {model_name} ({num_labels} labels)\")\n",
    "\n",
    "# 3. Setup training (just 50 steps!)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./smoke_test_output\",\n",
    "    max_steps=50,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=10,\n",
    "    save_steps=25,\n",
    "    fp16=True,  # Use mixed precision for speed\n",
    "    report_to=\"none\",  # Don't log to wandb yet\n",
    ")\n",
    "print(\"‚úì Training config ready\")\n",
    "\n",
    "# 4. Create collator\n",
    "collator = NERCollator(tokenizer=tokenizer, label_schema=label_schema)\n",
    "print(\"‚úì Collator ready\")\n",
    "\n",
    "# 5. Train!\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_dataset,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training 50 steps on 100 samples...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Smoke test complete! Your pipeline works on Kaggle!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Success!\n",
    "\n",
    "If you got here without errors, you're ready for real experiments!\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Summary\n",
    "\n",
    "| Dataset | Task Type | Samples | Status |\n",
    "|---------|-----------|---------|--------|\n",
    "| BC2GM | NER | 12,574 | ‚úÖ Active |\n",
    "| JNLPBA | NER | 18,607 | ‚úÖ Active |\n",
    "| ~~ChemProt~~ | ~~RE~~ | ~~1,020~~ | ‚ùå Disabled |\n",
    "| DDI | RE | 571 | ‚úÖ Active |\n",
    "| GAD | Classification | 3,836 | ‚úÖ Active |\n",
    "| HoC | Classification | 12,119 | ‚úÖ Active |\n",
    "| PubMedQA | QA | 800 | ‚úÖ Active |\n",
    "| BIOSSES | Similarity | 64 | ‚úÖ Active |\n",
    "\n",
    "**Total**: 48,571 samples across 7 diverse tasks\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Option 1: Run Contamination Check (2 hours)\n",
    "\n",
    "Before training, check if test data leaked into pre-training:\n",
    "\n",
    "```python\n",
    "!python scripts/run_contamination_check.py \\\n",
    "    --data_path data/raw \\\n",
    "    --output_dir contamination_results \\\n",
    "    --device cuda\n",
    "```\n",
    "\n",
    "### Option 2: Run First Baseline (1 hour)\n",
    "\n",
    "BERT baseline on BC2GM:\n",
    "\n",
    "```python\n",
    "!python scripts/run_baseline.py \\\n",
    "    --model bert-base-uncased \\\n",
    "    --task bc2gm \\\n",
    "    --epochs 3 \\\n",
    "    --batch_size 16\n",
    "```\n",
    "\n",
    "### Option 3: Run Full Experiment (4-6 hours)\n",
    "\n",
    "Single-task training on all tasks:\n",
    "\n",
    "```python\n",
    "!python scripts/run_experiment.py strategy=s1_single task=all\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Monitor GPU Usage\n",
    "\n",
    "Run this in a separate cell:\n",
    "\n",
    "```python\n",
    "!watch -n 5 nvidia-smi\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Troubleshooting\n",
    "\n",
    "**\"CUDA out of memory\"**:\n",
    "- Reduce `per_device_train_batch_size` to 4 or 2\n",
    "- Add `gradient_accumulation_steps=4` to simulate larger batch\n",
    "\n",
    "**\"ModuleNotFoundError\"**:\n",
    "- Make sure `sys.path.insert(0, \"src\")` is in the cell\n",
    "- Re-run the imports cell\n",
    "\n",
    "**Session disconnected**:\n",
    "- Your checkpoints are saved every 200 steps\n",
    "- Resume training from last checkpoint\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your experiments!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}