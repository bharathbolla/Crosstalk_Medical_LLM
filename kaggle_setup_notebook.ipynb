{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Medical Cross-Task Knowledge Transfer - Kaggle Setup\n\n**Project**: Medical NLP with Small Language Models  \n**Goal**: Study cross-task knowledge transfer in medical NLP tasks  \n**GPU**: T4 (16GB VRAM)  \n**Datasets**: ‚úÖ 7 datasets PRE-INCLUDED in repo (no download needed!)\n\n---\n\n## ‚úÖ Datasets Already Included!\n\nAll datasets are committed to the repo - just clone and go!\n\n**No more:**\n- ‚ùå Version conflicts\n- ‚ùå Download issues  \n- ‚ùå Library incompatibilities\n\n**Just:**\n- ‚úÖ Clone repo\n- ‚úÖ Start training!\n\n---\n\n## Setup Checklist\n\nBefore running this notebook:\n1. ‚úÖ Enable **GPU T4 x2** in Settings ‚Üí Accelerator\n2. ‚úÖ Enable **Internet** in Settings ‚Üí Internet (for model downloads only)\n3. ‚úÖ Set **Persistence** to \"Files only\" in Settings\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your GitHub repository\n",
    "!git clone https://github.com/bharathbolla/Crosstalk_Medical_LLM.git\n",
    "%cd Crosstalk_Medical_LLM\n",
    "\n",
    "# Verify structure\n",
    "print(\"\\nüìÅ Repository structure:\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n# Note: We can use latest versions since datasets are pre-included!\n!pip install -q transformers datasets evaluate wandb accelerate scikit-learn pyyaml\n\nprint(\"‚úÖ Dependencies installed!\")\nprint(\"   Using latest versions - no compatibility issues since we're not downloading datasets!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available! Check Settings ‚Üí Accelerator ‚Üí GPU T4 x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4Ô∏è‚É£ Verify Datasets (Instant!)\n\nAll datasets are already in the repo - just verify they loaded correctly."
  },
  {
   "cell_type": "code",
   "source": "from pathlib import Path\nfrom datasets import load_from_disk\n\n# Verify all 7 required datasets exist\ndata_path = Path(\"data/raw\")\n\nprint(\"üì¶ Verifying datasets...\\n\")\nprint(\"=\" * 60)\n\ndatasets_info = {\n    \"bc2gm\": \"Gene/protein NER\",\n    \"jnlpba\": \"Bio-entity NER\",\n    \"ddi\": \"Drug-drug interaction RE\",\n    \"gad\": \"Gene-disease association\",\n    \"hoc\": \"Cancer hallmarks classification\",\n    \"pubmedqa\": \"Medical QA\",\n    \"biosses\": \"Sentence similarity\"\n}\n\ntotal_samples = 0\nverified = 0\n\nfor name, description in datasets_info.items():\n    print(f\"\\nüì¶ {name.upper()} - {description}\")\n    \n    dataset_path = data_path / name\n    \n    if not dataset_path.exists():\n        print(f\"   ‚úó NOT FOUND at {dataset_path}\")\n        continue\n    \n    try:\n        # Load the dataset\n        dataset = load_from_disk(str(dataset_path))\n        \n        # Count samples\n        train_size = len(dataset[\"train\"]) if \"train\" in dataset else 0\n        total_samples += train_size\n        verified += 1\n        \n        # Show splits\n        splits = list(dataset.keys())\n        splits_info = \", \".join([f\"{s}: {len(dataset[s])}\" for s in splits])\n        \n        print(f\"   ‚úì Loaded successfully!\")\n        print(f\"   ‚úì Splits: {splits_info}\")\n        \n    except Exception as e:\n        print(f\"   ‚úó Error loading: {str(e)[:80]}\")\n\n# Summary\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"‚úÖ Verified: {verified}/7 datasets\")\nprint(f\"üìä Total training samples: {total_samples:,}\")\n\nif verified == 7:\n    print(\"\\nüéâ All datasets ready! You can start training immediately!\")\nelse:\n    print(f\"\\n‚ö†Ô∏è  Missing {7 - verified} dataset(s)\")\n    \nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\nfrom pathlib import Path\nimport subprocess\nimport sys\n\n# Create data directory\ndata_path = Path(\"data/raw\")\ndata_path.mkdir(parents=True, exist_ok=True)\n\nprint(\"üì• Downloading 7 medical NLP datasets from bigbio collection...\\n\")\nprint(\"=\" * 60)\n\n# IMPORTANT: Newer datasets library versions require this\nprint(\"‚öôÔ∏è  Setting up environment for bigbio datasets...\")\nprint(\"   (These datasets use custom loading scripts)\")\nprint()\n\n# Dataset configurations\ndatasets_config = {\n    \"bc2gm\": {\n        \"repo\": \"bigbio/blurb\",\n        \"config\": \"bc2gm\",\n        \"description\": \"Gene/protein NER from PubMed abstracts\"\n    },\n    \"jnlpba\": {\n        \"repo\": \"bigbio/blurb\",\n        \"config\": \"jnlpba\",\n        \"description\": \"Bio-entity NER (protein, DNA, RNA, cell line, cell type)\"\n    },\n    \"ddi\": {\n        \"repo\": \"bigbio/ddi_corpus\",\n        \"config\": \"ddi_corpus_source\",\n        \"description\": \"Drug-drug interaction extraction\"\n    },\n    \"gad\": {\n        \"repo\": \"bigbio/gad\",\n        \"config\": \"gad_blurb_bigbio_text\",\n        \"description\": \"Gene-disease association classification\"\n    },\n    \"hoc\": {\n        \"repo\": \"bigbio/hallmarks_of_cancer\",\n        \"config\": \"hallmarks_of_cancer_source\",\n        \"description\": \"Cancer hallmarks classification (multi-label)\"\n    },\n    \"pubmedqa\": {\n        \"repo\": \"bigbio/pubmed_qa\",\n        \"config\": \"pubmed_qa_labeled_fold0_source\",\n        \"description\": \"Medical question answering\"\n    },\n    \"biosses\": {\n        \"repo\": \"bigbio/biosses\",\n        \"config\": \"biosses_bigbio_pairs\",\n        \"description\": \"Biomedical sentence similarity\"\n    }\n}\n\ntotal_samples = 0\nsuccessful = 0\nfailed = []\n\nfor name, config in datasets_config.items():\n    print(f\"\\nüì¶ {name.upper()}\")\n    print(f\"   {config['description']}\")\n\n    try:\n        # CRITICAL FIX: Must use trust_remote_code=True for bigbio datasets\n        # These datasets have custom loading scripts that are safe but need explicit permission\n        dataset = load_dataset(\n            config[\"repo\"],\n            name=config[\"config\"],\n            trust_remote_code=True  # ‚ö†Ô∏è CHANGED from False to True - this is required!\n        )\n\n        # Save to disk\n        dataset.save_to_disk(str(data_path / name))\n\n        # Show stats\n        train_size = len(dataset[\"train\"])\n        total_samples += train_size\n        successful += 1\n\n        # Show split info\n        splits_info = \" + \".join([f\"{split}: {len(dataset[split])}\" for split in dataset.keys()])\n        print(f\"   ‚úì Downloaded! Splits: {splits_info}\")\n\n    except Exception as e:\n        error_msg = str(e)\n        failed.append(name)\n        \n        # Check if it's the \"dataset scripts no longer supported\" error\n        if \"Dataset scripts are no longer supported\" in error_msg:\n            print(f\"   ‚úó ERROR: Dataset scripts blocked by datasets library\")\n            print(f\"   üí° FIX: Need to downgrade datasets library\")\n            print(f\"          Run: pip install datasets==2.14.0\")\n        else:\n            print(f\"   ‚úó ERROR: {error_msg[:100]}\")\n        continue\n\n# Summary\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"‚úÖ Successfully downloaded: {successful}/7 datasets\")\nprint(f\"üìä Total training samples: {total_samples:,}\")\n\nif successful == 7:\n    print(\"\\nüéâ All datasets downloaded successfully!\")\n    print(f\"\\nDatasets saved in: {data_path.absolute()}\")\nelif successful == 0:\n    print(\"\\n‚ö†Ô∏è  FALLBACK NEEDED: All downloads failed!\")\n    print(\"\\nüîß FIX: Run this command in a cell BEFORE this one:\")\n    print(\"   !pip install -q datasets==2.14.0\")\n    print(\"\\nThen re-run this cell. The older datasets version supports custom scripts.\")\nelse:\n    print(f\"\\n‚ö†Ô∏è  Partial success - Failed datasets: {', '.join(failed)}\")\n    \nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Test Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that parsers work\n",
    "import sys\n",
    "sys.path.insert(0, \"src\")\n",
    "\n",
    "from data import TaskRegistry, BC2GMDataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Check registered tasks (should be 7 without ChemProt)\n",
    "print(f\"Registered tasks: {TaskRegistry.list_tasks()}\")\n",
    "\n",
    "# Load one dataset\n",
    "dataset = BC2GMDataset(\n",
    "    data_path=Path(\"data/raw\"),\n",
    "    split=\"train\"\n",
    ")\n",
    "print(f\"\\nLoaded {len(dataset)} BC2GM samples\")\n",
    "print(f\"First sample:\\n  {dataset[0].input_text[:150]}...\")\n",
    "\n",
    "# Check label schema\n",
    "schema = dataset.get_label_schema()\n",
    "print(f\"\\nLabel schema ({len(schema)} labels): {list(schema.keys())}\")\n",
    "\n",
    "print(\"\\n‚úÖ Everything works! Ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Smoke Test - Quick Training Test (10 minutes)\n",
    "\n",
    "Train BERT on 100 samples for 50 steps to verify the pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## üéâ Success!\n\nIf you got here without errors, you're ready for real experiments!\n\n---\n\n## Dataset Summary\n\nAll datasets are **pre-included** in the repository:\n\n| Dataset | Task Type | Train Samples | Status |\n|---------|-----------|---------------|--------|\n| BC2GM | NER | ~15,000 | ‚úÖ Included |\n| JNLPBA | NER | ~18,500 | ‚úÖ Included |\n| DDI | RE | ~2,900 | ‚úÖ Included |\n| GAD | Classification | ~4,200 | ‚úÖ Included |\n| HoC | Classification | ~1,500 | ‚úÖ Included |\n| PubMedQA | QA | ~450 | ‚úÖ Included |\n| BIOSSES | Similarity | 64 | ‚úÖ Included |\n\n**Total**: ~42,000 training samples across 7 diverse medical NLP tasks\n\n**Bonus**: Also includes bc5cdr and chemprot datasets!\n\n---\n\n## Next Steps\n\n### Option 1: Run Test Parsers (1 minute)\n\nVerify all parsers work correctly:\n\n```python\n!python test_parsers.py\n```\n\n### Option 2: Run Smoke Test (10 minutes)\n\nQuick training test on 100 samples - see Cell 6 above.\n\n### Option 3: Run First Baseline (1 hour)\n\nBERT baseline on BC2GM:\n\n```python\n!python scripts/run_baseline.py \\\n    --model bert-base-uncased \\\n    --task bc2gm \\\n    --epochs 3 \\\n    --batch_size 16\n```\n\n### Option 4: Run Full Experiment (4-6 hours)\n\nSingle-task training on all tasks:\n\n```python\n!python scripts/run_experiment.py strategy=s1_single task=all\n```\n\n---\n\n## üìä Monitor GPU Usage\n\nRun this in a separate cell:\n\n```python\n!watch -n 5 nvidia-smi\n```\n\n---\n\n## üîß Troubleshooting\n\n**\"ModuleNotFoundError\"**:\n- Make sure `sys.path.insert(0, \"src\")` is in the cell\n- Re-run the imports cell\n\n**\"CUDA out of memory\"**:\n- Reduce `per_device_train_batch_size` to 4 or 2\n- Add `gradient_accumulation_steps=4` to simulate larger batch\n\n**Session disconnected**:\n- Your checkpoints are saved every 200 steps\n- Resume training from last checkpoint\n\n**Dataset not found**:\n- Make sure you cloned the full repo with `git clone` (not downloaded as ZIP)\n- Check `data/raw/` directory exists\n\n---\n\n## üéØ Time Savings\n\n| Task | Old Method | New Method |\n|------|-----------|------------|\n| Setup | 8+ hours (with errors) | 2 minutes ‚úÖ |\n| Dataset download | Failed | Not needed ‚úÖ |\n| Version conflicts | Many | None ‚úÖ |\n| Total to first training | Never worked | 15 minutes ‚úÖ |\n\n---\n\n**Happy training!** üöÄ\n\nSee [DATASETS_INCLUDED.md](DATASETS_INCLUDED.md) for more details."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Success!\n",
    "\n",
    "If you got here without errors, you're ready for real experiments!\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Summary\n",
    "\n",
    "| Dataset | Task Type | Samples | Status |\n",
    "|---------|-----------|---------|--------|\n",
    "| BC2GM | NER | 12,574 | ‚úÖ Active |\n",
    "| JNLPBA | NER | 18,607 | ‚úÖ Active |\n",
    "| ~~ChemProt~~ | ~~RE~~ | ~~1,020~~ | ‚ùå Disabled |\n",
    "| DDI | RE | 571 | ‚úÖ Active |\n",
    "| GAD | Classification | 3,836 | ‚úÖ Active |\n",
    "| HoC | Classification | 12,119 | ‚úÖ Active |\n",
    "| PubMedQA | QA | 800 | ‚úÖ Active |\n",
    "| BIOSSES | Similarity | 64 | ‚úÖ Active |\n",
    "\n",
    "**Total**: 48,571 samples across 7 diverse tasks\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Option 1: Run Contamination Check (2 hours)\n",
    "\n",
    "Before training, check if test data leaked into pre-training:\n",
    "\n",
    "```python\n",
    "!python scripts/run_contamination_check.py \\\n",
    "    --data_path data/raw \\\n",
    "    --output_dir contamination_results \\\n",
    "    --device cuda\n",
    "```\n",
    "\n",
    "### Option 2: Run First Baseline (1 hour)\n",
    "\n",
    "BERT baseline on BC2GM:\n",
    "\n",
    "```python\n",
    "!python scripts/run_baseline.py \\\n",
    "    --model bert-base-uncased \\\n",
    "    --task bc2gm \\\n",
    "    --epochs 3 \\\n",
    "    --batch_size 16\n",
    "```\n",
    "\n",
    "### Option 3: Run Full Experiment (4-6 hours)\n",
    "\n",
    "Single-task training on all tasks:\n",
    "\n",
    "```python\n",
    "!python scripts/run_experiment.py strategy=s1_single task=all\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Monitor GPU Usage\n",
    "\n",
    "Run this in a separate cell:\n",
    "\n",
    "```python\n",
    "!watch -n 5 nvidia-smi\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Troubleshooting\n",
    "\n",
    "**\"CUDA out of memory\"**:\n",
    "- Reduce `per_device_train_batch_size` to 4 or 2\n",
    "- Add `gradient_accumulation_steps=4` to simulate larger batch\n",
    "\n",
    "**\"ModuleNotFoundError\"**:\n",
    "- Make sure `sys.path.insert(0, \"src\")` is in the cell\n",
    "- Re-run the imports cell\n",
    "\n",
    "**Session disconnected**:\n",
    "- Your checkpoints are saved every 200 steps\n",
    "- Resume training from last checkpoint\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your experiments!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}