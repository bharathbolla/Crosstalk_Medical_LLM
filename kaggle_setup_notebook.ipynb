{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Cross-Task Knowledge Transfer - Kaggle Setup\n",
    "\n",
    "**Project**: Medical NLP with Small Language Models  \n",
    "**Goal**: Study cross-task knowledge transfer in medical NLP tasks  \n",
    "**GPU**: T4 (16GB VRAM)  \n",
    "\n",
    "---\n",
    "\n",
    "## Setup Checklist\n",
    "\n",
    "Before running this notebook:\n",
    "1. ‚úÖ Enable **GPU T4 x2** in Settings ‚Üí Accelerator\n",
    "2. ‚úÖ Enable **Internet** in Settings ‚Üí Internet\n",
    "3. ‚úÖ Set **Persistence** to \"Files only\" in Settings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your GitHub repository\n",
    "!git clone https://github.com/bharathbolla/Crosstalk_Medical_LLM.git\n",
    "%cd Crosstalk_Medical_LLM\n",
    "\n",
    "# Verify structure\n",
    "print(\"\\nüìÅ Repository structure:\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets evaluate wandb accelerate scikit-learn pyyaml\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available! Check Settings ‚Üí Accelerator ‚Üí GPU T4 x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Download Datasets (15 minutes)\n",
    "\n",
    "Downloads all 8 medical NLP datasets from HuggingFace using Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Create data directory\n",
    "data_path = Path(\"data/raw\")\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset configurations\n",
    "datasets_config = {\n",
    "    \"bc2gm\": {\n",
    "        \"url\": \"https://huggingface.co/datasets/bigbio/blurb/resolve/refs%2Fconvert%2Fparquet/bc2gm\",\n",
    "        \"splits\": [\"train\", \"validation\", \"test\"]\n",
    "    },\n",
    "    \"jnlpba\": {\n",
    "        \"url\": \"https://huggingface.co/datasets/bigbio/blurb/resolve/refs%2Fconvert%2Fparquet/jnlpba\",\n",
    "        \"splits\": [\"train\", \"validation\", \"test\"]\n",
    "    },\n",
    "    \"chemprot\": {\n",
    "        \"url\": \"https://huggingface.co/datasets/bigbio/blurb/resolve/refs%2Fconvert%2Fparquet/chemprot\",\n",
    "        \"splits\": [\"train\", \"validation\", \"test\"]\n",
    "    },\n",
    "    \"ddi\": {\n",
    "        \"url\": \"https://huggingface.co/datasets/bigbio/blurb/resolve/refs%2Fconvert%2Fparquet/ddi_corpus\",\n",
    "        \"splits\": [\"train\", \"test\"]\n",
    "    },\n",
    "    \"gad\": {\n",
    "        \"url\": \"https://huggingface.co/datasets/bigbio/blurb/resolve/refs%2Fconvert%2Fparquet/gad\",\n",
    "        \"splits\": [\"train\", \"test\"]\n",
    "    },\n",
    "    \"hoc\": {\n",
    "        \"url\": \"https://huggingface.co/datasets/bigbio/blurb/resolve/refs%2Fconvert%2Fparquet/hallmarks_of_cancer\",\n",
    "        \"splits\": [\"train\", \"validation\", \"test\"]\n",
    "    },\n",
    "    \"pubmedqa\": {\n",
    "        \"url\": \"https://huggingface.co/datasets/bigbio/blurb/resolve/refs%2Fconvert%2Fparquet/pubmed_qa\",\n",
    "        \"splits\": [\"train\", \"validation\", \"test\"]\n",
    "    },\n",
    "    \"biosses\": {\n",
    "        \"url\": \"https://huggingface.co/datasets/bigbio/blurb/resolve/refs%2Fconvert%2Fparquet/biosses\",\n",
    "        \"splits\": [\"train\", \"validation\", \"test\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üì• Downloading 8 medical NLP datasets...\\n\")\n",
    "\n",
    "total_samples = 0\n",
    "for name, config in datasets_config.items():\n",
    "    print(f\"Downloading {name}...\")\n",
    "    base_url = config[\"url\"]\n",
    "    \n",
    "    # Build data_files dict\n",
    "    data_files = {}\n",
    "    for split in config[\"splits\"]:\n",
    "        data_files[split] = f\"{base_url}/{split}/0000.parquet\"\n",
    "    \n",
    "    # Load and save\n",
    "    dataset = load_dataset(\"parquet\", data_files=data_files)\n",
    "    dataset.save_to_disk(str(data_path / name))\n",
    "    \n",
    "    # Show stats\n",
    "    train_size = len(dataset[\"train\"])\n",
    "    total_samples += train_size\n",
    "    print(f\"  ‚úì {name}: {train_size:,} training samples\\n\")\n",
    "\n",
    "print(f\"‚úÖ All 8 datasets downloaded!\")\n",
    "print(f\"üìä Total training samples: {total_samples:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Test Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that parsers work\n",
    "import sys\n",
    "sys.path.insert(0, \"src\")\n",
    "\n",
    "from data import TaskRegistry, BC2GMDataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Check registered tasks\n",
    "print(f\"Registered tasks: {TaskRegistry.list_tasks()}\")\n",
    "\n",
    "# Load one dataset\n",
    "dataset = BC2GMDataset(\n",
    "    data_path=Path(\"data/raw\"),\n",
    "    split=\"train\"\n",
    ")\n",
    "print(f\"\\nLoaded {len(dataset)} BC2GM samples\")\n",
    "print(f\"First sample:\\n  {dataset[0].input_text[:150]}...\")\n",
    "\n",
    "# Check label schema\n",
    "schema = dataset.get_label_schema()\n",
    "print(f\"\\nLabel schema ({len(schema)} labels): {list(schema.keys())}\")\n",
    "\n",
    "print(\"\\n‚úÖ Everything works! Ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Smoke Test - Quick Training Test (10 minutes)\n",
    "\n",
    "Train BERT on 100 samples for 50 steps to verify the pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from src.data import BC2GMDataset\n",
    "from src.data.collators import NERCollator\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üöÄ Starting smoke test...\\n\")\n",
    "\n",
    "# 1. Load tiny subset (100 samples only)\n",
    "dataset = BC2GMDataset(data_path=Path(\"data/raw\"), split=\"train\")\n",
    "small_dataset = [dataset[i] for i in range(100)]\n",
    "print(f\"‚úì Loaded {len(small_dataset)} samples\")\n",
    "\n",
    "# 2. Load BERT model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"‚úì Loaded tokenizer: {model_name}\")\n",
    "\n",
    "label_schema = dataset.get_label_schema()\n",
    "num_labels = len(label_schema)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ").to(\"cuda\")\n",
    "print(f\"‚úì Loaded model: {model_name} ({num_labels} labels)\")\n",
    "\n",
    "# 3. Setup training (just 50 steps!)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./smoke_test_output\",\n",
    "    max_steps=50,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_steps=10,\n",
    "    save_steps=25,\n",
    "    fp16=True,  # Use mixed precision for speed\n",
    "    report_to=\"none\",  # Don't log to wandb yet\n",
    ")\n",
    "print(\"‚úì Training config ready\")\n",
    "\n",
    "# 4. Create collator\n",
    "collator = NERCollator(tokenizer=tokenizer, label_schema=label_schema)\n",
    "print(\"‚úì Collator ready\")\n",
    "\n",
    "# 5. Train!\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_dataset,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training 50 steps on 100 samples...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Smoke test complete! Your pipeline works on Kaggle!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Success!\n",
    "\n",
    "If you got here without errors, you're ready for real experiments!\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Option 1: Run Contamination Check (2 hours)\n",
    "\n",
    "Before training, check if test data leaked into pre-training:\n",
    "\n",
    "```python\n",
    "!python scripts/run_contamination_check.py \\\n",
    "    --data_path data/raw \\\n",
    "    --output_dir contamination_results \\\n",
    "    --device cuda\n",
    "```\n",
    "\n",
    "### Option 2: Run First Baseline (1 hour)\n",
    "\n",
    "BERT baseline on BC2GM:\n",
    "\n",
    "```python\n",
    "!python scripts/run_baseline.py \\\n",
    "    --model bert-base-uncased \\\n",
    "    --task bc2gm \\\n",
    "    --epochs 3 \\\n",
    "    --batch_size 16\n",
    "```\n",
    "\n",
    "### Option 3: Run Full Experiment (4-6 hours)\n",
    "\n",
    "Single-task training on all tasks:\n",
    "\n",
    "```python\n",
    "!python scripts/run_experiment.py strategy=s1_single task=all\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Monitor GPU Usage\n",
    "\n",
    "Run this in a separate cell:\n",
    "\n",
    "```python\n",
    "!watch -n 5 nvidia-smi\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Troubleshooting\n",
    "\n",
    "**\"CUDA out of memory\"**:\n",
    "- Reduce `per_device_train_batch_size` to 4 or 2\n",
    "- Add `gradient_accumulation_steps=4` to simulate larger batch\n",
    "\n",
    "**\"ModuleNotFoundError\"**:\n",
    "- Make sure `sys.path.insert(0, \"src\")` is in the cell\n",
    "- Re-run the imports cell\n",
    "\n",
    "**Session disconnected**:\n",
    "- Your checkpoints are saved every 200 steps\n",
    "- Resume training from last checkpoint\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your experiments!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
