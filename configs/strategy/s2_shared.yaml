strategy:
  name: "S2_shared_mtl"
  type: "multi_task"
  description: "Single shared LoRA adapter trained on all tasks."

  adapter:
    shared: true
    shared_rank: 16
    private: false

  multitask:
    sampling: "temperature"
    temperature: 2.0
    loss_weighting: "uncertainty"

  training:
    epochs: 5
    early_stopping_patience: 3
    eval_every_n_steps: 200
    checkpoint_every_n_steps: 200
    learning_rate: 2e-5
    warmup_ratio: 0.1

  logging:
    token_logging: true
    log_tokens_per_task: true
    log_cumulative_tokens: true
    wandb_project: "medical-mtl"
