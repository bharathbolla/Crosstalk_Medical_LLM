model:
  name: "google/gemma-2-2b"
  short_name: "gemma2b"
  params: 2.6e9

  quantization:
    enabled: false  # FP16 fits on T4
    bits: null
    quant_type: null
    compute_dtype: "float16"
    double_quant: false

  lora:
    rank: 16
    alpha: 32
    dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

  training:
    max_batch_size_t4: 16  # 2.6B fits larger batches
    gradient_checkpointing: false
    flash_attention: true

  unsloth:
    compatible: true
    use_for: ["S1", "S2"]
