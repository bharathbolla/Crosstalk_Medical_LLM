model:
  name: "meta-llama/Llama-3.2-3B"
  short_name: "llama3b"
  params: 3.2e9

  quantization:
    enabled: false  # FP16 fits on T4
    bits: null
    quant_type: null
    compute_dtype: "float16"
    double_quant: false

  lora:
    rank: 16
    alpha: 32
    dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

  training:
    max_batch_size_t4: 12
    gradient_checkpointing: false
    flash_attention: true

  unsloth:
    compatible: true
    use_for: ["S1", "S2"]
