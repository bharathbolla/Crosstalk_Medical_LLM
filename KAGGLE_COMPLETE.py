"""
Complete Kaggle Notebook Generator
Includes ALL cells: setup, config, smoke test, AND actual training/evaluation
"""

import sys
import io
import json

# Fix Windows encoding
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

def create_complete_notebook():
    """Generate complete notebook with all execution cells."""

    notebook = {
        "cells": [
            # ==================================================
            # TITLE
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "# Medical Multi-Task Learning: Complete Training Notebook\n",
                    "## ‚úÖ All 7 Models √ó All 8 Tasks - Ready to Train!\n",
                    "\n",
                    "**What This Notebook Does**:\n",
                    "- ‚úÖ Clones code from GitHub\n",
                    "- ‚úÖ Loads tokenizer, datasets, model automatically\n",
                    "- ‚úÖ Trains with proper configuration\n",
                    "- ‚úÖ Evaluates and saves results\n",
                    "- ‚úÖ Works with all 7 models and all 8 tasks\n",
                    "\n",
                    "**Expected Results**:\n",
                    "- BioBERT on BC2GM: **F1 = 0.84** (not 0.46!)\n",
                    "- Smoke test (50 samples): F1 > 0.30 in 2 minutes\n",
                    "- Full training: F1 = 0.84 in ~3 hours"
                ]
            },

            # ==================================================
            # CELL 1: SETUP
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## Cell 1: Setup & Clone Repository"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "import sys\n",
                    "import os\n",
                    "from pathlib import Path\n",
                    "\n",
                    "# Clone repo\n",
                    "print('üì• Cloning repository...')\n",
                    "os.chdir('/kaggle/working')\n",
                    "!rm -rf Crosstalk_Medical_LLM\n",
                    "!git clone https://github.com/bharathbolla/Crosstalk_Medical_LLM.git\n",
                    "os.chdir('Crosstalk_Medical_LLM')\n",
                    "\n",
                    "print(f'\\n‚úÖ Current directory: {os.getcwd()}')\n",
                    "\n",
                    "# Verify datasets exist\n",
                    "!python test_pickle_load.py"
                ]
            },

            # ==================================================
            # CELL 2: INSTALL
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## Cell 2: Install Dependencies"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "!pip install -q transformers torch accelerate scikit-learn seqeval pandas scipy\n",
                    "\n",
                    "import torch\n",
                    "import json\n",
                    "import pandas as pd\n",
                    "from datetime import datetime\n",
                    "from pathlib import Path\n",
                    "\n",
                    "# GPU verification\n",
                    "print(f'\\n‚úÖ PyTorch: {torch.__version__}')\n",
                    "print(f'‚úÖ CUDA: {torch.cuda.is_available()}')\n",
                    "if torch.cuda.is_available():\n",
                    "    print(f'‚úÖ GPU: {torch.cuda.get_device_name(0)}')\n",
                    "    print(f'‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
                    "\n",
                    "RESULTS_DIR = Path('results')\n",
                    "RESULTS_DIR.mkdir(exist_ok=True)\n",
                    "EXPERIMENT_ID = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
                    "print(f'\\nüìä Experiment ID: {EXPERIMENT_ID}')"
                ]
            },

            # ==================================================
            # CELL 3: CONFIGURATION
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Cell 3: Configuration\n",
                    "### ‚≠ê Change ONLY these 2 lines to test different models/tasks!"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# ============================================\n",
                    "# ‚≠ê MAIN CONFIGURATION\n",
                    "# ============================================\n",
                    "\n",
                    "CONFIG = {\n",
                    "    # ‚≠ê MODEL (choose one of 7 models)\n",
                    "    'model_name': 'dmis-lab/biobert-v1.1',  # BioBERT\n",
                    "    # Other options:\n",
                    "    # 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12',  # BlueBERT\n",
                    "    # 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',  # PubMedBERT\n",
                    "    # 'allenai/biomed_roberta_base',  # BioMed-RoBERTa\n",
                    "    # 'emilyalsentzer/Bio_ClinicalBERT',  # Clinical-BERT\n",
                    "    # 'roberta-base',  # RoBERTa\n",
                    "    # 'bert-base-uncased',  # BERT\n",
                    "    \n",
                    "    # ‚≠ê TASK (choose one or more)\n",
                    "    'datasets': ['bc2gm'],  # Start with BC2GM\n",
                    "    # Options: bc2gm, jnlpba, chemprot, ddi, gad, hoc, pubmedqa, biosses\n",
                    "    \n",
                    "    'experiment_id': EXPERIMENT_ID,\n",
                    "    'max_samples_per_dataset': None,\n",
                    "    'num_epochs': 10,\n",
                    "    'batch_size': 32,\n",
                    "    'learning_rate': 2e-5,\n",
                    "    'max_length': 512,\n",
                    "    'warmup_steps': 500,\n",
                    "    'use_early_stopping': True,\n",
                    "    'early_stopping_patience': 3,\n",
                    "}\n",
                    "\n",
                    "# Auto-adjust batch size based on GPU\n",
                    "if torch.cuda.is_available():\n",
                    "    gpu_name = torch.cuda.get_device_name(0)\n",
                    "    if 'A100' in gpu_name:\n",
                    "        CONFIG['batch_size'] = 64\n",
                    "    elif 'T4' in gpu_name:\n",
                    "        CONFIG['batch_size'] = 32\n",
                    "\n",
                    "print('='*60)\n",
                    "print('CONFIGURATION')\n",
                    "print('='*60)\n",
                    "print(f\"Model: {CONFIG['model_name']}\")\n",
                    "print(f\"Tasks: {CONFIG['datasets']}\")\n",
                    "print(f\"Batch: {CONFIG['batch_size']}\")\n",
                    "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
                    "print('='*60)"
                ]
            },

            # ==================================================
            # CELL 4: SMOKE TEST & MULTI-DATASET TESTING
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Cell 4: üî• SMOKE TEST & MULTI-DATASET VALIDATION\n",
                    "### Set SMOKE_TEST = True for 2-minute validation\n",
                    "### Set TEST_ALL_DATASETS = True to validate all 8 datasets"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# ============================================\n",
                    "# ‚≠ê SMOKE TEST & MULTI-DATASET TOGGLE\n",
                    "# ============================================\n",
                    "\n",
                    "SMOKE_TEST = True  # ‚Üê Change to False for full training\n",
                    "TEST_ALL_DATASETS = False  # ‚Üê Change to True to test all 8 datasets\n",
                    "\n",
                    "print('\\n' + '='*60)\n",
                    "if TEST_ALL_DATASETS:\n",
                    "    print('üî• MULTI-DATASET VALIDATION MODE')\n",
                    "    print('='*60)\n",
                    "    \n",
                    "    # All 8 datasets\n",
                    "    all_datasets = [\n",
                    "        'bc2gm',       # NER\n",
                    "        'jnlpba',      # NER\n",
                    "        'chemprot',    # RE\n",
                    "        'ddi',         # RE\n",
                    "        'gad',         # Classification\n",
                    "        'hoc',         # Multi-label\n",
                    "        'pubmedqa',    # QA\n",
                    "        'biosses',     # Similarity\n",
                    "    ]\n",
                    "    \n",
                    "    # Store for later\n",
                    "    CONFIG['all_datasets_to_test'] = all_datasets\n",
                    "    CONFIG['dataset_test_results'] = []\n",
                    "    \n",
                    "    # Smoke test config\n",
                    "    CONFIG['max_samples_per_dataset'] = 100  # 100 samples per dataset\n",
                    "    CONFIG['num_epochs'] = 3  # 3 epochs (quick test)\n",
                    "    CONFIG['batch_size'] = 16\n",
                    "    CONFIG['max_length'] = 256\n",
                    "    CONFIG['warmup_steps'] = 50\n",
                    "    CONFIG['use_early_stopping'] = False\n",
                    "    \n",
                    "    print(f'\\nDatasets to test: {len(all_datasets)}')\n",
                    "    for i, ds in enumerate(all_datasets, 1):\n",
                    "        task_type = TASK_CONFIGS[ds]['task_type']\n",
                    "        print(f'  {i}. {ds:12s} | {task_type}')\n",
                    "    \n",
                    "    print('\\n‚è±Ô∏è  Expected time: ~20-30 minutes')\n",
                    "    print('üìä Expected: All datasets train successfully')\n",
                    "\n",
                    "elif SMOKE_TEST:\n",
                    "    print('üî• SMOKE TEST MODE')\n",
                    "    print('='*60)\n",
                    "    CONFIG['max_samples_per_dataset'] = 50\n",
                    "    CONFIG['num_epochs'] = 1\n",
                    "    CONFIG['batch_size'] = 16\n",
                    "    CONFIG['max_length'] = 128\n",
                    "    CONFIG['use_early_stopping'] = False\n",
                    "    print('Settings: 50 samples, 1 epoch, batch 16')\n",
                    "    print('Expected: F1 > 0.30')\n",
                    "    print('Time: ~2 minutes')\n",
                    "else:\n",
                    "    print('üöÄ FULL TRAINING MODE')\n",
                    "    print('='*60)\n",
                    "    print(f\"Samples: ALL\")\n",
                    "    print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
                    "    print(f\"Batch: {CONFIG['batch_size']}\")\n",
                    "    print('Expected: F1 = 0.84')\n",
                    "    print('Time: ~3 hours')\n",
                    "print('='*60)"
                ]
            },

            # ==================================================
            # CELL 5: LOAD COMPLETE CODE
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Cell 5: Load Complete Implementation\n",
                    "### Imports all fixed code from repository"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "print('\\nüì¶ Loading complete implementation...')\n",
                    "\n",
                    "# Execute dataset code\n",
                    "exec(open('COMPLETE_FIXED_DATASET.py').read())\n",
                    "print('‚úÖ Dataset code loaded')\n",
                    "\n",
                    "# Execute model code\n",
                    "exec(open('COMPLETE_FIXED_MODEL.py').read())\n",
                    "print('‚úÖ Model code loaded')\n",
                    "\n",
                    "# Execute metrics code\n",
                    "exec(open('COMPLETE_FIXED_METRICS.py').read())\n",
                    "print('‚úÖ Metrics code loaded')\n",
                    "\n",
                    "print('\\n' + '='*60)\n",
                    "print('‚úÖ ALL CODE LOADED')\n",
                    "print('='*60)"
                ]
            },

            # ==================================================
            # CELL 6: LOAD TOKENIZER
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## Cell 6: Load Tokenizer"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "from transformers import AutoTokenizer\n",
                    "\n",
                    "print('\\nüî§ Loading tokenizer...')\n",
                    "\n",
                    "# Load tokenizer (handles RoBERTa automatically)\n",
                    "model_name = CONFIG['model_name']\n",
                    "\n",
                    "if 'roberta' in model_name.lower():\n",
                    "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
                    "    print('   ‚úÖ RoBERTa tokenizer (add_prefix_space=True)')\n",
                    "else:\n",
                    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                    "    print('   ‚úÖ BERT tokenizer')\n",
                    "\n",
                    "print(f'   Model: {model_name}')\n",
                    "print(f'   Vocab size: {tokenizer.vocab_size:,}')\n",
                    "print('='*60)"
                ]
            },

            # ==================================================
            # CELL 7: LOAD DATASETS
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## Cell 7: Load Datasets"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "import pickle\n",
                    "from pathlib import Path\n",
                    "\n",
                    "print('\\nüìä Loading datasets...')\n",
                    "\n",
                    "primary_dataset = CONFIG['datasets'][0]\n",
                    "max_samples = CONFIG['max_samples_per_dataset']\n",
                    "\n",
                    "# Load pickle file from correct path\n",
                    "pickle_file = Path('data/pickle') / f'{primary_dataset}.pkl'\n",
                    "print(f'   Loading from: {pickle_file}')\n",
                    "\n",
                    "with open(pickle_file, 'rb') as f:\n",
                    "    raw_data = pickle.load(f)\n",
                    "\n",
                    "# Some datasets use 'test' instead of 'validation'\n",
                    "val_split = 'validation' if 'validation' in raw_data else 'test'\n",
                    "if val_split not in raw_data:\n",
                    "    # For datasets with only train split (like pubmedqa)\n",
                    "    print('   ‚ö†Ô∏è  No validation split found, using train split for validation')\n",
                    "    raw_data[val_split] = raw_data['train'][-100:]  # Use last 100 samples\n",
                    "\n",
                    "# Limit samples if smoke test\n",
                    "if max_samples:\n",
                    "    raw_data['train'] = raw_data['train'][:max_samples]\n",
                    "    raw_data[val_split] = raw_data[val_split][:max_samples//5]\n",
                    "\n",
                    "print(f'   Dataset: {primary_dataset}')\n",
                    "print(f\"   Train samples: {len(raw_data['train']):,}\")\n",
                    "print(f\"   Validation samples: {len(raw_data[val_split]):,}\")\n",
                    "\n",
                    "# Create datasets using UniversalMedicalDataset\n",
                    "# Note: UniversalMedicalDataset looks up task_type and labels from TASK_CONFIGS\n",
                    "train_dataset = UniversalMedicalDataset(\n",
                    "    data=raw_data['train'],\n",
                    "    tokenizer=tokenizer,\n",
                    "    task_name=primary_dataset,  # Pass task name, not task_type!\n",
                    "    max_length=CONFIG['max_length']\n",
                    ")\n",
                    "\n",
                    "val_dataset = UniversalMedicalDataset(\n",
                    "    data=raw_data[val_split],\n",
                    "    tokenizer=tokenizer,\n",
                    "    task_name=primary_dataset,\n",
                    "    max_length=CONFIG['max_length']\n",
                    ")\n",
                    "\n",
                    "# Get task config for display\n",
                    "task_config = TASK_CONFIGS[primary_dataset]\n",
                    "\n",
                    "# Store dataset stats\n",
                    "dataset_stats = {\n",
                    "    primary_dataset: {\n",
                    "        'task_type': task_config['task_type'],\n",
                    "        'model_type': task_config['model_type'],\n",
                    "        'num_labels': len(task_config['labels']) if task_config['labels'] else 1,\n",
                    "        'train_size': len(train_dataset),\n",
                    "        'val_size': len(val_dataset),\n",
                    "    }\n",
                    "}\n",
                    "\n",
                    "print(f\"   ‚úÖ Created UniversalMedicalDataset\")\n",
                    "print(f\"   Task type: {task_config['task_type']}\")\n",
                    "print(f\"   Num labels: {dataset_stats[primary_dataset]['num_labels']}\")\n",
                    "print('='*60)"
                ]
            },

            # ==================================================
            # CELL 8: LOAD MODEL
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## Cell 8: Load Model"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "from transformers import (\n",
                    "    AutoModelForTokenClassification,\n",
                    "    AutoModelForSequenceClassification,\n",
                    "    AutoConfig\n",
                    ")\n",
                    "\n",
                    "print('\\nü§ñ Loading model...')\n",
                    "\n",
                    "# Load model with correct head for task\n",
                    "task_info = dataset_stats[primary_dataset]\n",
                    "model_type = task_info['model_type']\n",
                    "num_labels = task_info['num_labels']\n",
                    "\n",
                    "if model_type == 'token_classification':\n",
                    "    # NER tasks\n",
                    "    model = AutoModelForTokenClassification.from_pretrained(\n",
                    "        model_name,\n",
                    "        num_labels=num_labels,\n",
                    "        ignore_mismatched_sizes=True\n",
                    "    )\n",
                    "    print(f'   ‚úÖ TokenClassification head loaded')\n",
                    "\n",
                    "elif model_type == 'sequence_classification':\n",
                    "    # RE, Classification, QA\n",
                    "    config = AutoConfig.from_pretrained(model_name)\n",
                    "    if task_config.get('problem_type'):\n",
                    "        config.problem_type = task_config['problem_type']\n",
                    "    \n",
                    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
                    "        model_name,\n",
                    "        config=config,\n",
                    "        num_labels=num_labels,\n",
                    "        ignore_mismatched_sizes=True\n",
                    "    )\n",
                    "    print(f'   ‚úÖ SequenceClassification head loaded')\n",
                    "\n",
                    "elif model_type == 'regression':\n",
                    "    # Similarity\n",
                    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
                    "        model_name,\n",
                    "        num_labels=1,\n",
                    "        ignore_mismatched_sizes=True\n",
                    "    )\n",
                    "    print(f'   ‚úÖ Regression head loaded')\n",
                    "\n",
                    "# Move to GPU\n",
                    "if torch.cuda.is_available():\n",
                    "    model = model.cuda()\n",
                    "    print('   ‚úÖ Model on GPU')\n",
                    "\n",
                    "# Count parameters\n",
                    "total_params = sum(p.numel() for p in model.parameters())\n",
                    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                    "\n",
                    "print(f'   Total parameters: {total_params:,}')\n",
                    "print(f'   Trainable: {trainable_params:,} ({100 * trainable_params / total_params:.1f}%)')\n",
                    "print('='*60)"
                ]
            },

            # ==================================================
            # CELL 9: SETUP TRAINER
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## Cell 9: Setup Trainer"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "from transformers import (\n",
                    "    TrainingArguments,\n",
                    "    Trainer,\n",
                    "    EarlyStoppingCallback\n",
                    ")\n",
                    "\n",
                    "print('\\n‚öôÔ∏è  Setting up trainer...')\n",
                    "\n",
                    "# Training arguments\n",
                    "training_args = TrainingArguments(\n",
                    "    output_dir=f\"./checkpoints/{primary_dataset}_{EXPERIMENT_ID}\",\n",
                    "    num_train_epochs=CONFIG['num_epochs'],\n",
                    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
                    "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
                    "    learning_rate=CONFIG['learning_rate'],\n",
                    "    warmup_steps=CONFIG['warmup_steps'],\n",
                    "    weight_decay=0.01,\n",
                    "    logging_dir='./logs',\n",
                    "    logging_steps=50,\n",
                    "    eval_strategy='epoch',\n",
                    "    save_strategy='epoch',\n",
                    "    load_best_model_at_end=True,\n",
                    "    metric_for_best_model='f1',\n",
                    "    greater_is_better=True,\n",
                    "    save_total_limit=2,\n",
                    "    report_to='none',\n",
                    "    disable_tqdm=False,\n",
                    ")\n",
                    "\n",
                    "# Setup compute_metrics function\n",
                    "if task_config['task_type'] == 'ner':\n",
                    "    def compute_metrics_fn(eval_pred):\n",
                    "        predictions = eval_pred.predictions\n",
                    "        labels = eval_pred.label_ids\n",
                    "        return compute_ner_metrics(predictions, labels, task_config['labels'])\n",
                    "elif task_config['task_type'] in ['re', 'classification', 'qa']:\n",
                    "    def compute_metrics_fn(eval_pred):\n",
                    "        predictions = eval_pred.predictions\n",
                    "        labels = eval_pred.label_ids\n",
                    "        return compute_classification_metrics(predictions, labels)\n",
                    "elif task_config['task_type'] == 'multilabel_classification':\n",
                    "    def compute_metrics_fn(eval_pred):\n",
                    "        predictions = eval_pred.predictions\n",
                    "        labels = eval_pred.label_ids\n",
                    "        return compute_multilabel_metrics(predictions, labels)\n",
                    "elif task_config['task_type'] == 'similarity':\n",
                    "    def compute_metrics_fn(eval_pred):\n",
                    "        predictions = eval_pred.predictions\n",
                    "        labels = eval_pred.label_ids\n",
                    "        return compute_regression_metrics(predictions, labels)\n",
                    "\n",
                    "# Create trainer\n",
                    "callbacks = []\n",
                    "if CONFIG['use_early_stopping']:\n",
                    "    callbacks.append(EarlyStoppingCallback(early_stopping_patience=CONFIG['early_stopping_patience']))\n",
                    "\n",
                    "trainer = Trainer(\n",
                    "    model=model,\n",
                    "    args=training_args,\n",
                    "    train_dataset=train_dataset,\n",
                    "    eval_dataset=val_dataset,\n",
                    "    tokenizer=tokenizer,\n",
                    "    compute_metrics=compute_metrics_fn,\n",
                    "    callbacks=callbacks,\n",
                    ")\n",
                    "\n",
                    "print('   ‚úÖ Trainer ready')\n",
                    "print(f'   Output: {training_args.output_dir}')\n",
                    "print('='*60)"
                ]
            },

            # ==================================================
            # CELL 10: TRAIN MODEL (with Multi-Dataset Support)
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Cell 10: Train Model\n",
                    "### üöÄ This is where the actual training happens!\n",
                    "### Supports both single dataset and multi-dataset testing"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "import gc\n",
                    "from transformers import (\n",
                    "    AutoModelForTokenClassification,\n",
                    "    AutoModelForSequenceClassification,\n",
                    "    AutoConfig\n",
                    ")\n",
                    "\n",
                    "if 'all_datasets_to_test' in CONFIG:\n",
                    "    # ============================================\n",
                    "    # MULTI-DATASET TRAINING LOOP\n",
                    "    # ============================================\n",
                    "    print('\\n' + '='*60)\n",
                    "    print('üöÄ MULTI-DATASET TRAINING LOOP')\n",
                    "    print('='*60)\n",
                    "    \n",
                    "    all_results = []\n",
                    "    \n",
                    "    for ds_idx, dataset_to_test in enumerate(CONFIG['all_datasets_to_test'], 1):\n",
                    "        print(f'\\n{\"=\"*60}')\n",
                    "        print(f'DATASET {ds_idx}/{len(CONFIG[\"all_datasets_to_test\"])}: {dataset_to_test.upper()}')\n",
                    "        print(f'{\"=\"*60}')\n",
                    "        \n",
                    "        try:\n",
                    "            # Update current dataset\n",
                    "            primary_dataset = dataset_to_test\n",
                    "            task_config = TASK_CONFIGS[primary_dataset]\n",
                    "            \n",
                    "            print(f'Task type: {task_config[\"task_type\"]}')\n",
                    "            print(f'Model type: {task_config[\"model_type\"]}')\n",
                    "            \n",
                    "            # Step 1: Load data\n",
                    "            print(f'\\nüìä Loading {primary_dataset} data...')\n",
                    "            pickle_file = Path('data/pickle') / f'{primary_dataset}.pkl'\n",
                    "            \n",
                    "            with open(pickle_file, 'rb') as f:\n",
                    "                raw_data = pickle.load(f)\n",
                    "            \n",
                    "            # Handle different splits\n",
                    "            val_split = 'validation' if 'validation' in raw_data else 'test'\n",
                    "            if val_split not in raw_data:\n",
                    "                raw_data[val_split] = raw_data['train'][-20:]\n",
                    "            \n",
                    "            # Limit samples with leakage-free random validation\n",
                    "            import random\n",
                    "            random.seed(42)  # Reproducible\n",
                    "            if CONFIG['max_samples_per_dataset']:\n",
                    "                max_train = CONFIG['max_samples_per_dataset']\n",
                    "                num_val = max(20, CONFIG['max_samples_per_dataset'] // 5)\n",
                    "                raw_data['train'] = raw_data['train'][:max_train]\n",
                    "                \n",
                    "                # Random sample from FULL validation set for label diversity\n",
                    "                full_val = raw_data[val_split]\n",
                    "                if len(full_val) > num_val:\n",
                    "                    raw_data[val_split] = random.sample(full_val, num_val)\n",
                    "                # else: keep full val set as-is\n",
                    "            \n",
                    "            print(f\"   Train samples: {len(raw_data['train']):,}\")\n",
                    "            print(f\"   Val samples: {len(raw_data[val_split]):,}\")\n",
                    "            \n",
                    "            # DIAGNOSTIC: Check validation set for issues\n",
                    "            print(f'\\nüîç Validation Set Diagnostics:')\n",
                    "            \n",
                    "            # Check label distribution\n",
                    "            from collections import Counter\n",
                    "            val_labels = []\n",
                    "            for sample in raw_data[val_split]:\n",
                    "                label = sample.get('label', sample.get('relation', sample.get('ner_tags', None)))\n",
                    "                if label is not None:\n",
                    "                    val_labels.append(str(label))\n",
                    "            \n",
                    "            if val_labels:\n",
                    "                label_counts = Counter(val_labels)\n",
                    "                print(f'   Unique labels in val: {len(label_counts)}')\n",
                    "                most_common = label_counts.most_common(1)[0]\n",
                    "                imbalance_ratio = most_common[1] / len(val_labels)\n",
                    "                print(f'   Most common label: {most_common[0]} ({most_common[1]}/{len(val_labels)} = {imbalance_ratio*100:.1f}%)')\n",
                    "                if imbalance_ratio > 0.90:\n",
                    "                    print(f'   ‚ö†Ô∏è  WARNING: Highly imbalanced validation set!')\n",
                    "            \n",
                    "            # Check for text overlap (data leakage)\n",
                    "            train_texts = set(s.get('text', '')[:100] for s in raw_data['train'] if s.get('text'))\n",
                    "            val_texts = [s.get('text', '')[:100] for s in raw_data[val_split] if s.get('text')]\n",
                    "            overlapping = sum(1 for vt in val_texts if vt in train_texts)\n",
                    "            if overlapping > 0:\n",
                    "                print(f'   ‚ö†Ô∏è  DATA LEAKAGE: {overlapping}/{len(val_texts)} val samples overlap with train!')\n",
                    "            \n",
                    "            # Check for empty texts\n",
                    "            empty_val = sum(1 for s in raw_data[val_split] if not s.get('text', '').strip())\n",
                    "            if empty_val > 0:\n",
                    "                print(f'   ‚ö†Ô∏è  EMPTY TEXT: {empty_val}/{len(raw_data[val_split])} val samples have empty text!')\n",
                    "            \n",
                    "            # Print first val sample for inspection\n",
                    "            if len(raw_data[val_split]) > 0:\n",
                    "                first_sample = raw_data[val_split][0]\n",
                    "                print(f'   Sample val text: {first_sample.get(\"text\", \"\")[:100]}...')\n",
                    "                print(f'   Sample val label: {first_sample.get(\"label\", first_sample.get(\"relation\", \"N/A\"))}')\n",
                    "            \n",
                    "            # Step 2: Load tokenizer\n",
                    "            print(f'\\nüî§ Loading tokenizer...')\n",
                    "            model_name = CONFIG['model_name']\n",
                    "            \n",
                    "            if 'roberta' in model_name.lower():\n",
                    "                tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
                    "            else:\n",
                    "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                    "            \n",
                    "            # Step 3: Create datasets\n",
                    "            print(f'\\nüì¶ Creating datasets...')\n",
                    "            train_dataset = UniversalMedicalDataset(\n",
                    "                data=raw_data['train'],\n",
                    "                tokenizer=tokenizer,\n",
                    "                task_name=primary_dataset,\n",
                    "                max_length=CONFIG['max_length']\n",
                    "            )\n",
                    "            \n",
                    "            val_dataset = UniversalMedicalDataset(\n",
                    "                data=raw_data[val_split],\n",
                    "                tokenizer=tokenizer,\n",
                    "                task_name=primary_dataset,\n",
                    "                max_length=CONFIG['max_length']\n",
                    "            )\n",
                    "            \n",
                    "            # Prepare dataset stats\n",
                    "            dataset_stats = {\n",
                    "                primary_dataset: {\n",
                    "                    'task_type': task_config['task_type'],\n",
                    "                    'model_type': task_config['model_type'],\n",
                    "                    'num_labels': len(task_config['labels']) if task_config['labels'] else 1,\n",
                    "                    'train_size': len(train_dataset),\n",
                    "                    'val_size': len(val_dataset),\n",
                    "                }\n",
                    "            }\n",
                    "            \n",
                    "            print(f'   ‚úÖ Train: {len(train_dataset)}, Val: {len(val_dataset)}')\n",
                    "            \n",
                    "            # Step 4: Load model\n",
                    "            print(f'\\nü§ñ Loading model...')\n",
                    "            task_info = dataset_stats[primary_dataset]\n",
                    "            model_type = task_info['model_type']\n",
                    "            num_labels = task_info['num_labels']\n",
                    "            \n",
                    "            if model_type == 'token_classification':\n",
                    "                # NER tasks\n",
                    "                model = AutoModelForTokenClassification.from_pretrained(\n",
                    "                    model_name,\n",
                    "                    num_labels=num_labels,\n",
                    "                    ignore_mismatched_sizes=True\n",
                    "                )\n",
                    "                print(f'   ‚úÖ TokenClassification head loaded ({num_labels} labels)')\n",
                    "            \n",
                    "            elif model_type == 'sequence_classification':\n",
                    "                # RE, Classification, QA\n",
                    "                config = AutoConfig.from_pretrained(model_name)\n",
                    "                config.num_labels = num_labels  # Set num_labels in config\n",
                    "                if task_config.get('problem_type'):\n",
                    "                    config.problem_type = task_config['problem_type']\n",
                    "                \n",
                    "                model = AutoModelForSequenceClassification.from_pretrained(\n",
                    "                    model_name,\n",
                    "                    config=config,\n",
                    "                    ignore_mismatched_sizes=True\n",
                    "                )\n",
                    "                print(f'   ‚úÖ SequenceClassification head loaded ({num_labels} labels)')\n",
                    "            \n",
                    "            elif model_type == 'regression':\n",
                    "                # Similarity\n",
                    "                model = AutoModelForSequenceClassification.from_pretrained(\n",
                    "                    model_name,\n",
                    "                    num_labels=1,\n",
                    "                    ignore_mismatched_sizes=True\n",
                    "                )\n",
                    "                print(f'   ‚úÖ Regression head loaded')\n",
                    "            \n",
                    "            if torch.cuda.is_available():\n",
                    "                model = model.cuda()\n",
                    "                print('   ‚úÖ Model on GPU')\n",
                    "            \n",
                    "            # Step 5: Setup trainer\n",
                    "            print(f'\\n‚öôÔ∏è  Setting up trainer...')\n",
                    "            training_args = TrainingArguments(\n",
                    "                output_dir=f\"./checkpoints/{primary_dataset}_{EXPERIMENT_ID}\",\n",
                    "                num_train_epochs=CONFIG['num_epochs'],\n",
                    "                per_device_train_batch_size=CONFIG['batch_size'],\n",
                    "                per_device_eval_batch_size=CONFIG['batch_size'],\n",
                    "                learning_rate=CONFIG['learning_rate'],\n",
                    "                warmup_steps=CONFIG['warmup_steps'],\n",
                    "                weight_decay=0.01,\n",
                    "                logging_dir='./logs',\n",
                    "                logging_steps=20,\n",
                    "                eval_strategy='epoch',\n",
                    "                save_strategy='no',  # Don't save to save disk space\n",
                    "                report_to='none',\n",
                    "                disable_tqdm=False,\n",
                    "            )\n",
                    "            \n",
                    "            # Setup metrics\n",
                    "            if task_config['task_type'] == 'ner':\n",
                    "                def compute_metrics_fn(eval_pred):\n",
                    "                    predictions = eval_pred.predictions\n",
                    "                    labels = eval_pred.label_ids\n",
                    "                    return compute_ner_metrics(predictions, labels, task_config['labels'])\n",
                    "            elif task_config['task_type'] in ['re', 'classification', 'qa']:\n",
                    "                def compute_metrics_fn(eval_pred):\n",
                    "                    predictions = eval_pred.predictions\n",
                    "                    labels = eval_pred.label_ids\n",
                    "                    return compute_classification_metrics(predictions, labels)\n",
                    "            elif task_config['task_type'] == 'multilabel_classification':\n",
                    "                def compute_metrics_fn(eval_pred):\n",
                    "                    predictions = eval_pred.predictions\n",
                    "                    labels = eval_pred.label_ids\n",
                    "                    return compute_multilabel_metrics(predictions, labels)\n",
                    "            elif task_config['task_type'] == 'similarity':\n",
                    "                def compute_metrics_fn(eval_pred):\n",
                    "                    predictions = eval_pred.predictions\n",
                    "                    labels = eval_pred.label_ids\n",
                    "                    return compute_regression_metrics(predictions, labels)\n",
                    "            \n",
                    "            trainer = Trainer(\n",
                    "                model=model,\n",
                    "                args=training_args,\n",
                    "                train_dataset=train_dataset,\n",
                    "                eval_dataset=val_dataset,\n",
                    "                tokenizer=tokenizer,\n",
                    "                compute_metrics=compute_metrics_fn,\n",
                    "            )\n",
                    "            \n",
                    "            print('   ‚úÖ Trainer ready')\n",
                    "            \n",
                    "            # Step 6: Train!\n",
                    "            print(f'\\nüöÄ Training {primary_dataset}...')\n",
                    "            start_time = datetime.now()\n",
                    "            \n",
                    "            train_result = trainer.train()\n",
                    "            \n",
                    "            elapsed = (datetime.now() - start_time).total_seconds()\n",
                    "            \n",
                    "            # Step 7: Evaluate\n",
                    "            print(f'\\nüìä Evaluating...')\n",
                    "            eval_result = trainer.evaluate()\n",
                    "            \n",
                    "            # Collect results\n",
                    "            result = {\n",
                    "                'dataset': primary_dataset,\n",
                    "                'task_type': task_config['task_type'],\n",
                    "                'model_type': task_config['model_type'],\n",
                    "                'train_samples': len(raw_data['train']),\n",
                    "                'val_samples': len(raw_data[val_split]),\n",
                    "                'num_epochs': CONFIG['num_epochs'],\n",
                    "                'f1': eval_result.get('eval_f1', 0),\n",
                    "                'precision': eval_result.get('eval_precision', 0),\n",
                    "                'recall': eval_result.get('eval_recall', 0),\n",
                    "                'accuracy': eval_result.get('eval_accuracy', 0),\n",
                    "                'train_loss': train_result.training_loss,\n",
                    "                'eval_loss': eval_result.get('eval_loss', 0),\n",
                    "                'time_seconds': elapsed,\n",
                    "            }\n",
                    "            \n",
                    "            # Handle regression (BIOSSES)\n",
                    "            if 'eval_pearson' in eval_result:\n",
                    "                result['pearson'] = eval_result['eval_pearson']\n",
                    "                result['mse'] = eval_result.get('eval_mse', 0)\n",
                    "            \n",
                    "            # Determine status\n",
                    "            if result['f1'] > 0.30:\n",
                    "                result['status'] = 'PASS'\n",
                    "            elif 'pearson' in result and result['pearson'] > 0.50:\n",
                    "                result['status'] = 'PASS'\n",
                    "            else:\n",
                    "                result['status'] = 'FAIL'\n",
                    "            \n",
                    "            all_results.append(result)\n",
                    "            \n",
                    "            # Print result\n",
                    "            print(f'\\n{\"=\"*60}')\n",
                    "            print(f'‚úÖ {primary_dataset.upper()} COMPLETE')\n",
                    "            print(f'{\"=\"*60}')\n",
                    "            if result['f1'] > 0:\n",
                    "                print(f'F1: {result[\"f1\"]:.4f}')\n",
                    "                print(f'Precision: {result[\"precision\"]:.4f}')\n",
                    "                print(f'Recall: {result[\"recall\"]:.4f}')\n",
                    "            if 'pearson' in result:\n",
                    "                print(f'Pearson: {result[\"pearson\"]:.4f}')\n",
                    "            print(f'Time: {elapsed:.1f}s')\n",
                    "            print(f'Status: {result[\"status\"]} {\"‚úÖ\" if result[\"status\"] == \"PASS\" else \"‚ùå\"}')\n",
                    "            \n",
                    "            # Clean up GPU memory\n",
                    "            del model, trainer, train_dataset, val_dataset, tokenizer\n",
                    "            torch.cuda.empty_cache()\n",
                    "            gc.collect()\n",
                    "            \n",
                    "        except Exception as e:\n",
                    "            print(f'\\n‚ùå ERROR with {dataset_to_test}:')\n",
                    "            print(f'   {str(e)}')\n",
                    "            import traceback\n",
                    "            traceback.print_exc()\n",
                    "            \n",
                    "            # Record error\n",
                    "            all_results.append({\n",
                    "                'dataset': dataset_to_test,\n",
                    "                'task_type': TASK_CONFIGS.get(dataset_to_test, {}).get('task_type', 'unknown'),\n",
                    "                'model_type': TASK_CONFIGS.get(dataset_to_test, {}).get('model_type', 'unknown'),\n",
                    "                'train_samples': 0,\n",
                    "                'val_samples': 0,\n",
                    "                'num_epochs': CONFIG['num_epochs'],\n",
                    "                'f1': 0.0,\n",
                    "                'precision': 0.0,\n",
                    "                'recall': 0.0,\n",
                    "                'accuracy': 0.0,\n",
                    "                'train_loss': 0.0,\n",
                    "                'eval_loss': 0.0,\n",
                    "                'time_seconds': 0,\n",
                    "                'status': 'ERROR',\n",
                    "                'error': str(e)\n",
                    "            })\n",
                    "            \n",
                    "            # Continue to next dataset\n",
                    "            continue\n",
                    "    \n",
                    "    # ============================================\n",
                    "    # SUMMARY OF ALL RESULTS\n",
                    "    # ============================================\n",
                    "    \n",
                    "    print('\\n' + '='*60)\n",
                    "    print('üìä MULTI-DATASET VALIDATION SUMMARY')\n",
                    "    print('='*60)\n",
                    "    \n",
                    "    # Create DataFrame\n",
                    "    df_results = pd.DataFrame(all_results)\n",
                    "    \n",
                    "    # Display results\n",
                    "    print('\\nResults:')\n",
                    "    print(df_results[['dataset', 'task_type', 'f1', 'precision', 'recall', 'time_seconds', 'status']].to_string(index=False))\n",
                    "    \n",
                    "    # Count by status\n",
                    "    passed = sum(1 for r in all_results if r['status'] == 'PASS')\n",
                    "    failed = sum(1 for r in all_results if r['status'] == 'FAIL')\n",
                    "    errors = sum(1 for r in all_results if r['status'] == 'ERROR')\n",
                    "    \n",
                    "    print(f'\\n{\"=\"*60}')\n",
                    "    print(f'Passed: {passed}/{len(all_results)} ‚úÖ')\n",
                    "    print(f'Failed: {failed}/{len(all_results)} ‚ùå')\n",
                    "    print(f'Errors: {errors}/{len(all_results)} üí•')\n",
                    "    print(f'{\"=\"*60}')\n",
                    "    \n",
                    "    # Group by task type\n",
                    "    print('\\nüìä Results by Task Type:')\n",
                    "    for task_type in df_results['task_type'].unique():\n",
                    "        task_df = df_results[df_results['task_type'] == task_type]\n",
                    "        avg_f1 = task_df['f1'].mean()\n",
                    "        count = len(task_df)\n",
                    "        print(f'  {task_type:25s} | Avg F1: {avg_f1:.4f} | Datasets: {count}')\n",
                    "    \n",
                    "    # Save results\n",
                    "    results_file = RESULTS_DIR / f'dataset_validation_{EXPERIMENT_ID}.csv'\n",
                    "    df_results.to_csv(results_file, index=False)\n",
                    "    print(f'\\nüíæ Results saved to: {results_file}')\n",
                    "    \n",
                    "    # Final verdict\n",
                    "    if failed == 0 and errors == 0:\n",
                    "        print('\\nüéâ ALL DATASETS PASSED!')\n",
                    "        print('‚úÖ Ready for full experiments (7 models √ó 8 tasks)')\n",
                    "    else:\n",
                    "        print(f'\\n‚ö†Ô∏è  {failed + errors} datasets had issues')\n",
                    "        print('Review errors above')\n",
                    "    \n",
                    "    # Store results\n",
                    "    CONFIG['dataset_test_results'] = all_results\n",
                    "\n",
                    "else:\n",
                    "    # ============================================\n",
                    "    # SINGLE DATASET TRAINING (ORIGINAL BEHAVIOR)\n",
                    "    # ============================================\n",
                    "    print('\\n' + '='*60)\n",
                    "    print('üöÄ STARTING TRAINING')\n",
                    "    print('='*60)\n",
                    "    \n",
                    "    # Train!\n",
                    "    train_result = trainer.train()\n",
                    "    \n",
                    "    print('\\n' + '='*60)\n",
                    "    print('‚úÖ TRAINING COMPLETE')\n",
                    "    print('='*60)\n",
                    "    print(f\"Train loss: {train_result.training_loss:.4f}\")\n",
                    "    print(f\"Train time: {train_result.metrics['train_runtime']:.1f}s\")\n",
                    "    print('='*60)"
                ]
            },

            # ==================================================
            # CELL 11: EVALUATE MODEL (Single Dataset Only)
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## Cell 11: Evaluate Model (Skip if multi-dataset mode)"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "if 'all_datasets_to_test' not in CONFIG:\n",
                    "    # Only evaluate for single dataset training\n",
                    "    print('\\nüìä Evaluating on validation set...')\n",
                    "    \n",
                    "    # Evaluate\n",
                    "    eval_result = trainer.evaluate()\n",
                    "    \n",
                    "    print('\\n' + '='*60)\n",
                    "    print('üìä EVALUATION RESULTS')\n",
                    "    print('='*60)\n",
                    "    for key, value in eval_result.items():\n",
                    "        if 'f1' in key.lower() or 'precision' in key.lower() or 'recall' in key.lower():\n",
                    "            print(f'{key}: {value:.4f}')\n",
                    "    print('='*60)\n",
                    "    \n",
                    "    # Save results\n",
                    "    results = {\n",
                    "        'experiment_id': EXPERIMENT_ID,\n",
                    "        'model': model_name,\n",
                    "        'dataset': primary_dataset,\n",
                    "        'task_type': task_config['task_type'],\n",
                    "        'smoke_test': SMOKE_TEST,\n",
                    "        'config': CONFIG,\n",
                    "        'train_metrics': train_result.metrics,\n",
                    "        'eval_metrics': eval_result,\n",
                    "    }\n",
                    "    \n",
                    "    # Save to JSON\n",
                    "    results_file = RESULTS_DIR / f'results_{EXPERIMENT_ID}.json'\n",
                    "    with open(results_file, 'w') as f:\n",
                    "        json.dump(results, f, indent=2)\n",
                    "    \n",
                    "    print(f'\\n‚úÖ Results saved to: {results_file}')\n",
                    "else:\n",
                    "    print('\\n‚è≠Ô∏è  Skipping - Multi-dataset results already saved in Cell 10')"
                ]
            },

            # ==================================================
            # CELL 12: FINAL SUMMARY
            # ==================================================
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## Cell 12: Final Summary"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "print('\\n' + '='*60)\n",
                    "print('üéâ EXPERIMENT COMPLETE')\n",
                    "print('='*60)\n",
                    "\n",
                    "if 'all_datasets_to_test' in CONFIG:\n",
                    "    # Multi-dataset summary\n",
                    "    print(f\"Mode: MULTI-DATASET VALIDATION\")\n",
                    "    print(f\"Model: {CONFIG['model_name']}\")\n",
                    "    print(f\"Datasets tested: {len(CONFIG['all_datasets_to_test'])}\")\n",
                    "    \n",
                    "    if 'dataset_test_results' in CONFIG and CONFIG['dataset_test_results']:\n",
                    "        passed = sum(1 for r in CONFIG['dataset_test_results'] if r['status'] == 'PASS')\n",
                    "        total = len(CONFIG['dataset_test_results'])\n",
                    "        print(f\"\\nPassed: {passed}/{total}\")\n",
                    "        \n",
                    "        if passed == total:\n",
                    "            print('\\n‚úÖ ALL DATASETS PASSED!')\n",
                    "            print('   ‚Üí Ready for full experiments (7 models √ó 8 tasks)')\n",
                    "        else:\n",
                    "            print(f'\\n‚ö†Ô∏è  {total - passed} datasets failed')\n",
                    "            print('   ‚Üí Review results above and fix issues')\n",
                    "    \n",
                    "    print('\\n' + '='*60)\n",
                    "    print('Next Steps:')\n",
                    "    print('1. Review dataset_validation_*.csv in results/ folder')\n",
                    "    print('2. Set TEST_ALL_DATASETS = False for normal training')\n",
                    "    print('3. Run full experiments if all datasets passed')\n",
                    "\n",
                    "else:\n",
                    "    # Single dataset summary\n",
                    "    print(f\"Model: {model_name}\")\n",
                    "    print(f\"Dataset: {primary_dataset}\")\n",
                    "    print(f\"Mode: {'SMOKE TEST' if SMOKE_TEST else 'FULL TRAINING'}\")\n",
                    "    print(f\"\\nF1 Score: {eval_result.get('eval_f1', 0):.4f}\")\n",
                    "    \n",
                    "    if SMOKE_TEST:\n",
                    "        if eval_result.get('eval_f1', 0) > 0.30:\n",
                    "            print('\\n‚úÖ Smoke test PASSED!')\n",
                    "            print('   ‚Üí Set SMOKE_TEST = False for full training')\n",
                    "        else:\n",
                    "            print('\\n‚ùå Smoke test FAILED')\n",
                    "            print('   ‚Üí Check configuration and data')\n",
                    "    else:\n",
                    "        expected_f1 = 0.84 if primary_dataset == 'bc2gm' else 0.70\n",
                    "        if eval_result.get('eval_f1', 0) > expected_f1 - 0.05:\n",
                    "            print(f'\\n‚úÖ Result matches expected F1 (~{expected_f1:.2f})')\n",
                    "        else:\n",
                    "            print(f'\\n‚ö†Ô∏è  F1 lower than expected (~{expected_f1:.2f})')\n",
                    "    \n",
                    "    print('\\n' + '='*60)\n",
                    "    print('Next Steps:')\n",
                    "    if SMOKE_TEST:\n",
                    "        print('1. Set SMOKE_TEST = False in Cell 4')\n",
                    "        print('2. Or set TEST_ALL_DATASETS = True to test all 8 datasets')\n",
                    "        print('3. Run All Cells')\n",
                    "    else:\n",
                    "        print('1. Try different models (change model_name in Cell 3)')\n",
                    "        print('2. Try different tasks (change datasets in Cell 3)')\n",
                    "        print('3. Check results/ folder for saved metrics')\n",
                    "\n",
                    "print('='*60)"
                ]
            },
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "name": "python",
                "version": "3.8.10"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }

    # Save
    with open('KAGGLE_COMPLETE.ipynb', 'w', encoding='utf-8') as f:
        json.dump(notebook, f, indent=2, ensure_ascii=False)

    print('‚úÖ Complete notebook created: KAGGLE_COMPLETE.ipynb')
    print('\nIncludes ALL cells:')
    print('  1. Setup & clone')
    print('  2. Install dependencies')
    print('  3. Configuration (change 2 lines)')
    print('  4. Smoke test toggle')
    print('  5. Load code')
    print('  6. Load tokenizer')
    print('  7. Load datasets')
    print('  8. Load model')
    print('  9. Setup trainer')
    print('  10. Train model ‚Üê ACTUAL TRAINING HAPPENS HERE')
    print('  11. Evaluate')
    print('  12. Summary')
    print('\nJust run all cells and it will train automatically!')

if __name__ == '__main__':
    create_complete_notebook()
