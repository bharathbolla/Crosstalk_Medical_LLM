# Fixes for ALL 8 Datasets + ALL 7 Models

## ‚úÖ **What's Already Fixed**

### **All 7 Models** ‚úÖ
- ‚úÖ BERT-based (5 models): Works with standard fixes
- ‚úÖ RoBERTa-based (2 models): Works with `add_prefix_space=True`

### **NER Tasks** ‚úÖ (BC2GM, JNLPBA)
- ‚úÖ Tokenization: `is_split_into_words=True`
- ‚úÖ Label alignment: `word_ids()` method
- ‚úÖ All 3 critical fixes applied
- ‚úÖ Expected F1: 0.84 (BioBERT on BC2GM)

---

## ‚ö†Ô∏è **What Needs Task-Specific Modifications**

### **Task Categories**

Your 8 datasets fall into 4 task types:

1. **NER** (2 datasets): BC2GM, JNLPBA ‚úÖ **FIXED**
2. **Relation Extraction** (2 datasets): ChemProt, DDI ‚ö†Ô∏è **Need RE modifications**
3. **Classification** (2 datasets): GAD, HoC ‚ö†Ô∏è **Need classification modifications**
4. **QA/Similarity** (2 datasets): PubMedQA, BIOSSES ‚ö†Ô∏è **Need QA/regression modifications**

---

## üõ†Ô∏è **Fixes by Task Type**

### **1. NER Tasks** ‚úÖ **COMPLETE**

**Datasets**: BC2GM, JNLPBA

**Status**: ‚úÖ All fixes already provided in FIXED_CELL_8.py

**Works with**:
- All 7 models
- Fixed tokenization (`is_split_into_words=True`)
- Fixed label alignment (`word_ids()`)

**Model head**:
```python
model = AutoModelForTokenClassification.from_pretrained(
    model_name,
    num_labels=num_labels  # BC2GM: 3, JNLPBA: 11
)
```

---

### **2. Relation Extraction Tasks** ‚ö†Ô∏è **NEEDS MODIFICATION**

**Datasets**: ChemProt, DDI

**Current issue**: Your notebook uses `AutoModelForTokenClassification` which is for NER, not RE!

**What needs to change**:

#### **Option A: Simple approach (for single-task)**

Treat RE as **sequence classification**:

```python
from transformers import AutoModelForSequenceClassification

# Load model for sequence classification
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_relation_types  # ChemProt: 13 relations, DDI: 4 types
)

# Tokenization fix (same as NER)
encoding = tokenizer(
    tokens,
    is_split_into_words=True,  # Still needed!
    max_length=512,
    padding='max_length',
    truncation=True,
    return_tensors='pt'
)

# No label alignment needed (sequence-level labels)
# Labels are at sentence level, not token level
labels = item['relation_label']  # Single label per sample
```

#### **Option B: Entity-pair approach (better for multi-task)**

Use entity markers:

```python
# Add special markers around entities
# Example: "[E1]aspirin[/E1] interacts with [E2]warfarin[/E2]"

tokens_with_markers = add_entity_markers(tokens, entity1_span, entity2_span)

encoding = tokenizer(
    tokens_with_markers,
    is_split_into_words=True,
    max_length=512,
    ...
)

# Use [CLS] representation for classification
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=13)
```

**For now**: Use Option A (simpler). Your FIXED_CELL_8.py tokenization still works!

---

### **3. Classification Tasks** ‚ö†Ô∏è **NEEDS MODIFICATION**

**Datasets**: GAD (binary), HoC (multi-label)

**What needs to change**:

```python
from transformers import AutoModelForSequenceClassification

# GAD: Binary classification (gene-disease association: yes/no)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2  # Binary
)

# HoC: Multi-label classification (10 hallmarks)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=10,
    problem_type="multi_label_classification"  # Key!
)

# Tokenization fix (same as before!)
encoding = tokenizer(
    text,  # For classification, can use text directly (not token list)
    max_length=512,
    padding='max_length',
    truncation=True,
    return_tensors='pt'
)

# Labels: sentence-level, no alignment needed
labels = item['label']  # Single label (GAD) or list of labels (HoC)
```

**Key difference**: No token-level labels, so **no word_ids() alignment needed**!

---

### **4. QA/Similarity Tasks** ‚ö†Ô∏è **NEEDS MODIFICATION**

**Datasets**: PubMedQA (QA), BIOSSES (similarity)

#### **PubMedQA** (Question Answering):

```python
from transformers import AutoModelForSequenceClassification

# PubMedQA: 3-way classification (yes/no/maybe)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3
)

# Tokenization: question + context pairs
encoding = tokenizer(
    question,
    context,
    max_length=512,
    padding='max_length',
    truncation='only_second',  # Truncate context, not question
    return_tensors='pt'
)

labels = item['answer']  # 0=no, 1=yes, 2=maybe
```

#### **BIOSSES** (Sentence Similarity):

```python
from transformers import AutoModelForSequenceClassification

# BIOSSES: Regression (similarity score 0-4)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=1  # Regression
)

# Tokenization: sentence pairs
encoding = tokenizer(
    sentence1,
    sentence2,
    max_length=512,
    padding='max_length',
    truncation=True,
    return_tensors='pt'
)

labels = item['similarity_score']  # Float 0-4

# Use MSE loss for regression
from torch.nn import MSELoss
loss_fn = MSELoss()
```

---

## üìä **Summary: What Works Where**

| Fix | NER | RE | Classification | QA/Similarity |
|-----|-----|----|--------------|--------------|
| **Model switch** (BERT‚ÜíBioBERT) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Tokenization** (`is_split_into_words`) | ‚úÖ | ‚úÖ (if using token-level) | ‚ùå (use text directly) | ‚ùå (use text pairs) |
| **Label alignment** (`word_ids()`) | ‚úÖ | ‚ùå (sentence-level labels) | ‚ùå (sentence-level labels) | ‚ùå (sentence-level labels) |
| **Model head** | TokenClassification | SequenceClassification | SequenceClassification | SequenceClassification |

---

## üéØ **Recommended Approach**

### **Phase 1: Fix NER Tasks FIRST** (BC2GM, JNLPBA)

‚úÖ **Status**: All fixes already provided!

**Why**: These are most critical for your paper's NER focus

**Action**:
1. Apply fixes from FIXED_CELL_8.py
2. Test BioBERT on BC2GM (expect F1 0.84)
3. Test BioBERT on JNLPBA (expect F1 0.78)
4. Test all 7 models on both NER datasets

**Timeline**: 3 hours/model √ó 7 models √ó 2 datasets = 42 hours

---

### **Phase 2: Add Task-Specific Modifications** (Other 6 datasets)

‚ö†Ô∏è **Status**: Need to modify notebook for each task type

**Option A: Separate notebooks per task type** (Recommended for clarity)
- notebook_ner.ipynb ‚úÖ (current, with fixes)
- notebook_re.ipynb (add RE modifications)
- notebook_classification.ipynb (add classification modifications)
- notebook_qa.ipynb (add QA/similarity modifications)

**Option B: Unified notebook with task detection** (Complex but flexible)
```python
# Detect task type and use appropriate model/head
task_type = detect_task_type(dataset_name)

if task_type == 'ner':
    model = AutoModelForTokenClassification(...)
    # Use word_ids() alignment
elif task_type == 're' or task_type == 'classification':
    model = AutoModelForSequenceClassification(...)
    # No word_ids() needed
elif task_type == 'regression':
    model = AutoModelForSequenceClassification(..., num_labels=1)
    # Use MSE loss
```

---

## üöÄ **Immediate Action Plan**

### **Today: Fix NER (BC2GM, JNLPBA)** ‚úÖ

1. Apply all 3 fixes from FIXED_CELL_8.py
2. Test BioBERT on BC2GM ‚Üí expect F1 0.84
3. If works, test all 7 models on BC2GM
4. Then test all 7 models on JNLPBA

**Success criteria**: BioBERT F1 ‚â• 0.84 on BC2GM

---

### **This Week: Add Other Task Types** ‚ö†Ô∏è

#### **Day 1-2: NER** (done above)

#### **Day 3: Relation Extraction** (ChemProt, DDI)
```python
# Main change: Use SequenceClassification head
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=13  # ChemProt
)
# Keep tokenization fix (is_split_into_words=True)
# Remove word_ids() alignment (sentence-level labels)
```

#### **Day 4: Classification** (GAD, HoC)
```python
# GAD: Binary
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2
)

# HoC: Multi-label
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=10,
    problem_type="multi_label_classification"
)
```

#### **Day 5: QA/Similarity** (PubMedQA, BIOSSES)
```python
# PubMedQA: 3-way classification
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3
)

# BIOSSES: Regression
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=1
)
```

---

## üìù **What You Need to Do**

### **For NER (BC2GM, JNLPBA)**: ‚úÖ **READY TO GO**

‚úÖ All fixes provided in:
- FIXED_CELL_8.py (complete code)
- QUICK_FIX_CARD.md (what to change)
- FIX_F1_ISSUE.md (why it's needed)

**Action**: Apply fixes and test!

---

### **For Other Tasks**: ‚ö†Ô∏è **Need task-specific notebooks**

Would you like me to:

1. ‚úÖ **Create task-specific dataset classes** for RE/Classification/QA?
2. ‚úÖ **Modify your notebook to auto-detect task type**?
3. ‚úÖ **Create separate notebooks for each task type**?
4. ‚úÖ **Prioritize NER first, then add others later**? (Recommended)

---

## üéØ **My Recommendation**

**START WITH NER ONLY** (BC2GM, JNLPBA):

**Why**:
1. You're getting F1 = 0.46 on BC2GM ‚Üí **fix this critical issue first**
2. NER is most important for your research focus
3. All fixes ready to apply immediately
4. Can verify fixes work before expanding to other tasks

**Timeline**:
- Today: Apply NER fixes, test BioBERT (3 hours)
- This week: Test all 7 models on both NER datasets (40 hours)
- Next week: Add other task types if needed

**Once NER works (F1 = 0.84)**, then we can add task-specific modifications for the other 6 datasets.

---

**Should I create the task-specific modifications now, or focus on getting NER working first?**
