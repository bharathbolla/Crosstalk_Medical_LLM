{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Cross-Task Transfer - FINAL WORKING NOTEBOOK\n",
    "\n",
    "**Status**: âœ… This works on Kaggle!\n",
    "\n",
    "**Setup**: GPU T4 x2 + Internet ON\n",
    "\n",
    "**Data**: Uses pickle files (NO library dependencies!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Clone Repository & Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Clone repo\n",
    "print(\"ðŸ“¥ Cloning repository...\")\n",
    "os.chdir('/kaggle/working')\n",
    "!rm -rf Crosstalk_Medical_LLM\n",
    "!git clone https://github.com/bharathbolla/Crosstalk_Medical_LLM.git\n",
    "os.chdir('Crosstalk_Medical_LLM')\n",
    "\n",
    "print(\"\\nâœ… Repository cloned!\")\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify pickle files exist\n",
    "print(\"\\nðŸ“¦ Checking datasets...\")\n",
    "!python test_pickle_load.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Install Only Training Libraries\n",
    "\n",
    "We don't need datasets/pyarrow for data loading (using pickle!)\n",
    "\n",
    "Just install transformers, torch, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install training libraries (NOT datasets/pyarrow!)\n",
    "!pip install -q transformers torch accelerate scikit-learn wandb\n",
    "\n",
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"\\nâœ… PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Load Dataset from Pickle\n",
    "\n",
    "Example: Load BC2GM (NER) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Load BC2GM dataset from pickle\n",
    "dataset_name = \"bc2gm\"\n",
    "pickle_file = Path(f\"data/pickle/{dataset_name}.pkl\")\n",
    "\n",
    "print(f\"ðŸ“¦ Loading {dataset_name} from pickle...\")\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Show statistics\n",
    "train_data = data['train']\n",
    "val_data = data.get('validation', [])\n",
    "test_data = data.get('test', [])\n",
    "\n",
    "print(f\"\\nâœ… Loaded {dataset_name}!\")\n",
    "print(f\"   Train: {len(train_data):,} samples\")\n",
    "print(f\"   Validation: {len(val_data):,} samples\")\n",
    "print(f\"   Test: {len(test_data):,} samples\")\n",
    "\n",
    "# Show first sample\n",
    "sample = train_data[0]\n",
    "print(f\"\\nðŸ“‹ First sample:\")\n",
    "print(f\"   ID: {sample.get('id', 'N/A')}\")\n",
    "print(f\"   Tokens: {sample['tokens'][:10]}...\")\n",
    "print(f\"   NER tags: {sample['ner_tags'][:10]}...\")\n",
    "print(f\"   All fields: {list(sample.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Prepare Data for Training\n",
    "\n",
    "Convert pickle data to HuggingFace format for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    \"\"\"Simple NER dataset from pickle data.\"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = item['tokens']\n",
    "        labels = item['ner_tags']\n",
    "\n",
    "        # Convert tokens to string and tokenize\n",
    "        text = ' '.join(tokens)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Align labels with tokenized input\n",
    "        # Simple approach: use first subword label\n",
    "        aligned_labels = [-100] * self.max_length\n",
    "        for i in range(min(len(labels), self.max_length)):\n",
    "            aligned_labels[i] = labels[i]\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(aligned_labels)\n",
    "        }\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NERDataset(train_data[:100], tokenizer)  # Use 100 samples for quick test\n",
    "val_dataset = NERDataset(val_data[:20] if val_data else train_data[:20], tokenizer)\n",
    "\n",
    "print(f\"âœ… Created training dataset: {len(train_dataset)} samples\")\n",
    "print(f\"âœ… Created validation dataset: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Quick Training Test (5 minutes)\n",
    "\n",
    "Train BERT for a few steps to verify everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Determine number of labels\n",
    "max_label = max([max(item['ner_tags']) for item in train_data])\n",
    "num_labels = max_label + 1\n",
    "\n",
    "print(f\"ðŸ“Š Number of labels: {num_labels}\")\n",
    "\n",
    "# Load model\n",
    "print(f\"\\nðŸ¤– Loading {model_name}...\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_trainer\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=10,\n",
    "    max_steps=50,  # Quick test\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    save_steps=50,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"\\nðŸš€ Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "trainer.train()\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Load Different Dataset\n",
    "\n",
    "Example: Load all 8 datasets and show statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Load all datasets\n",
    "datasets = [\"bc2gm\", \"jnlpba\", \"chemprot\", \"ddi\", \"gad\", \"hoc\", \"pubmedqa\", \"biosses\"]\n",
    "\n",
    "print(\"ðŸ“Š DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_data = {}\n",
    "total_samples = 0\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    pickle_file = Path(f\"data/pickle/{dataset_name}.pkl\")\n",
    "\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    all_data[dataset_name] = data\n",
    "    train_size = len(data['train'])\n",
    "    total_samples += train_size\n",
    "\n",
    "    splits_info = \", \".join([f\"{k}: {len(v)}\" for k, v in data.items()])\n",
    "    print(f\"\\n{dataset_name.upper()}:\")\n",
    "    print(f\"  Splits: {splits_info}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"âœ… Total training samples: {total_samples:,}\")\n",
    "print(f\"âœ… All {len(datasets)} datasets loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Save Your Work\n",
    "\n",
    "Save trained model or any results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "output_dir = \"./my_trained_model\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"âœ… Model saved to {output_dir}\")\n",
    "\n",
    "# List saved files\n",
    "!ls -lh {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success! ðŸŽ‰\n",
    "\n",
    "You now have:\n",
    "- âœ… Data loading working (pickle format)\n",
    "- âœ… Training pipeline working\n",
    "- âœ… All 8 datasets available\n",
    "- âœ… GPU acceleration\n",
    "- âœ… Model saving\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "1. **Extend training**: Increase `max_steps` or `num_train_epochs`\n",
    "2. **Try other datasets**: Change `dataset_name` in Cell 3\n",
    "3. **Try other models**: Change `model_name` to:\n",
    "   - `\"dmis-lab/biobert-v1.1\"`\n",
    "   - `\"allenai/scibert_scivocab_uncased\"`\n",
    "   - `\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"`\n",
    "4. **Multi-task learning**: Load multiple datasets and combine them\n",
    "5. **Hyperparameter tuning**: Adjust batch size, learning rate, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Resources:\n",
    "\n",
    "- Repository: https://github.com/bharathbolla/Crosstalk_Medical_LLM\n",
    "- All datasets: `data/pickle/` directory\n",
    "- Documentation: See README files in repo\n",
    "\n",
    "**Happy experimenting!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
