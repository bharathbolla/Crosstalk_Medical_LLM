{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Medical Multi-Task Learning: Complete Training Notebook\n",
        "## ‚úÖ All 7 Models √ó All 8 Tasks - Ready to Train!\n",
        "\n",
        "**What This Notebook Does**:\n",
        "- ‚úÖ Clones code from GitHub\n",
        "- ‚úÖ Loads tokenizer, datasets, model automatically\n",
        "- ‚úÖ Trains with proper configuration\n",
        "- ‚úÖ Evaluates and saves results\n",
        "- ‚úÖ Works with all 7 models and all 8 tasks\n",
        "\n",
        "**Expected Results**:\n",
        "- BioBERT on BC2GM: **F1 = 0.84** (not 0.46!)\n",
        "- Smoke test (50 samples): F1 > 0.30 in 2 minutes\n",
        "- Full training: F1 = 0.84 in ~3 hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Setup & Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Clone repo\n",
        "print('üì• Cloning repository...')\n",
        "os.chdir('/kaggle/working')\n",
        "!rm -rf Crosstalk_Medical_LLM\n",
        "!git clone https://github.com/bharathbolla/Crosstalk_Medical_LLM.git\n",
        "os.chdir('Crosstalk_Medical_LLM')\n",
        "\n",
        "print(f'\\n‚úÖ Current directory: {os.getcwd()}')\n",
        "\n",
        "# Verify datasets exist\n",
        "!python test_pickle_load.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers torch accelerate scikit-learn seqeval pandas scipy\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# GPU verification\n",
        "print(f'\\n‚úÖ PyTorch: {torch.__version__}')\n",
        "print(f'‚úÖ CUDA: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'‚úÖ GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
        "\n",
        "RESULTS_DIR = Path('results')\n",
        "RESULTS_DIR.mkdir(exist_ok=True)\n",
        "EXPERIMENT_ID = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "print(f'\\nüìä Experiment ID: {EXPERIMENT_ID}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Configuration\n",
        "### ‚≠ê Change ONLY these 2 lines to test different models/tasks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ‚≠ê MAIN CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "CONFIG = {\n",
        "    # ‚≠ê MODEL (choose one of 7 models)\n",
        "    'model_name': 'dmis-lab/biobert-v1.1',  # BioBERT\n",
        "    # Other options:\n",
        "    # 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12',  # BlueBERT\n",
        "    # 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',  # PubMedBERT\n",
        "    # 'allenai/biomed_roberta_base',  # BioMed-RoBERTa\n",
        "    # 'emilyalsentzer/Bio_ClinicalBERT',  # Clinical-BERT\n",
        "    # 'roberta-base',  # RoBERTa\n",
        "    # 'bert-base-uncased',  # BERT\n",
        "    \n",
        "    # ‚≠ê TASK (choose one or more)\n",
        "    'datasets': ['bc2gm'],  # Start with BC2GM\n",
        "    # Options: bc2gm, jnlpba, chemprot, ddi, gad, hoc, pubmedqa, biosses\n",
        "    \n",
        "    'experiment_id': EXPERIMENT_ID,\n",
        "    'max_samples_per_dataset': None,\n",
        "    'num_epochs': 10,\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 2e-5,\n",
        "    'max_length': 512,\n",
        "    'warmup_steps': 500,\n",
        "    'use_early_stopping': True,\n",
        "    'early_stopping_patience': 3,\n",
        "}\n",
        "\n",
        "# Auto-adjust batch size based on GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    if 'A100' in gpu_name:\n",
        "        CONFIG['batch_size'] = 64\n",
        "    elif 'T4' in gpu_name:\n",
        "        CONFIG['batch_size'] = 32\n",
        "\n",
        "print('='*60)\n",
        "print('CONFIGURATION')\n",
        "print('='*60)\n",
        "print(f\"Model: {CONFIG['model_name']}\")\n",
        "print(f\"Tasks: {CONFIG['datasets']}\")\n",
        "print(f\"Batch: {CONFIG['batch_size']}\")\n",
        "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: üî• SMOKE TEST (Run This First!)\n",
        "### Set SMOKE_TEST = True for 2-minute validation\n",
        "### Set SMOKE_TEST = False for full training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ‚≠ê SMOKE TEST TOGGLE\n",
        "# ============================================\n",
        "\n",
        "SMOKE_TEST = True  # ‚Üê Change to False for full training\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "if SMOKE_TEST:\n",
        "    print('üî• SMOKE TEST MODE')\n",
        "    print('='*60)\n",
        "    CONFIG['max_samples_per_dataset'] = 50\n",
        "    CONFIG['num_epochs'] = 1\n",
        "    CONFIG['batch_size'] = 16\n",
        "    CONFIG['max_length'] = 128\n",
        "    CONFIG['use_early_stopping'] = False\n",
        "    print('Settings: 50 samples, 1 epoch, batch 16')\n",
        "    print('Expected: F1 > 0.30')\n",
        "    print('Time: ~2 minutes')\n",
        "else:\n",
        "    print('üöÄ FULL TRAINING MODE')\n",
        "    print('='*60)\n",
        "    print(f\"Samples: ALL\")\n",
        "    print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
        "    print(f\"Batch: {CONFIG['batch_size']}\")\n",
        "    print('Expected: F1 = 0.84')\n",
        "    print('Time: ~3 hours')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Load Complete Implementation\n",
        "### Imports all fixed code from repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nüì¶ Loading complete implementation...')\n",
        "\n",
        "# Execute dataset code\n",
        "exec(open('COMPLETE_FIXED_DATASET.py').read())\n",
        "print('‚úÖ Dataset code loaded')\n",
        "\n",
        "# Execute model code\n",
        "exec(open('COMPLETE_FIXED_MODEL.py').read())\n",
        "print('‚úÖ Model code loaded')\n",
        "\n",
        "# Execute metrics code\n",
        "exec(open('COMPLETE_FIXED_METRICS.py').read())\n",
        "print('‚úÖ Metrics code loaded')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('‚úÖ ALL CODE LOADED')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Load Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "print('\\nüî§ Loading tokenizer...')\n",
        "\n",
        "# Load tokenizer (handles RoBERTa automatically)\n",
        "model_name = CONFIG['model_name']\n",
        "\n",
        "if 'roberta' in model_name.lower():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
        "    print('   ‚úÖ RoBERTa tokenizer (add_prefix_space=True)')\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print('   ‚úÖ BERT tokenizer')\n",
        "\n",
        "print(f'   Model: {model_name}')\n",
        "print(f'   Vocab size: {tokenizer.vocab_size:,}')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "print('\\nüìä Loading datasets...')\n",
        "\n",
        "primary_dataset = CONFIG['datasets'][0]\n",
        "max_samples = CONFIG['max_samples_per_dataset']\n",
        "\n",
        "# Load pickle file\n",
        "pickle_file = f'data/{primary_dataset}_train.pkl'\n",
        "with open(pickle_file, 'rb') as f:\n",
        "    raw_data = pickle.load(f)\n",
        "\n",
        "# Limit samples if smoke test\n",
        "if max_samples:\n",
        "    raw_data['train'] = raw_data['train'][:max_samples]\n",
        "    raw_data['validation'] = raw_data['validation'][:max_samples//5]\n",
        "\n",
        "print(f'   Dataset: {primary_dataset}')\n",
        "print(f\"   Train samples: {len(raw_data['train']):,}\")\n",
        "print(f\"   Validation samples: {len(raw_data['validation']):,}\")\n",
        "\n",
        "# Create datasets using UniversalMedicalDataset\n",
        "task_config = TASK_CONFIGS[primary_dataset]\n",
        "\n",
        "train_dataset = UniversalMedicalDataset(\n",
        "    data=raw_data['train'],\n",
        "    tokenizer=tokenizer,\n",
        "    task_type=task_config['task_type'],\n",
        "    labels=task_config['labels'],\n",
        "    max_length=CONFIG['max_length']\n",
        ")\n",
        "\n",
        "val_dataset = UniversalMedicalDataset(\n",
        "    data=raw_data['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    task_type=task_config['task_type'],\n",
        "    labels=task_config['labels'],\n",
        "    max_length=CONFIG['max_length']\n",
        ")\n",
        "\n",
        "# Store dataset stats\n",
        "dataset_stats = {\n",
        "    primary_dataset: {\n",
        "        'task_type': task_config['task_type'],\n",
        "        'model_type': task_config['model_type'],\n",
        "        'num_labels': len(task_config['labels']) if task_config['labels'] else 1,\n",
        "        'train_size': len(train_dataset),\n",
        "        'val_size': len(val_dataset),\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"   ‚úÖ Created UniversalMedicalDataset\")\n",
        "print(f\"   Task type: {task_config['task_type']}\")\n",
        "print(f\"   Num labels: {dataset_stats[primary_dataset]['num_labels']}\")\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoConfig\n",
        ")\n",
        "\n",
        "print('\\nü§ñ Loading model...')\n",
        "\n",
        "# Load model with correct head for task\n",
        "task_info = dataset_stats[primary_dataset]\n",
        "model_type = task_info['model_type']\n",
        "num_labels = task_info['num_labels']\n",
        "\n",
        "if model_type == 'token_classification':\n",
        "    # NER tasks\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    print(f'   ‚úÖ TokenClassification head loaded')\n",
        "\n",
        "elif model_type == 'sequence_classification':\n",
        "    # RE, Classification, QA\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    if task_config.get('problem_type'):\n",
        "        config.problem_type = task_config['problem_type']\n",
        "    \n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        config=config,\n",
        "        num_labels=num_labels,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    print(f'   ‚úÖ SequenceClassification head loaded')\n",
        "\n",
        "elif model_type == 'regression':\n",
        "    # Similarity\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=1,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    print(f'   ‚úÖ Regression head loaded')\n",
        "\n",
        "# Move to GPU\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    print('   ‚úÖ Model on GPU')\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'   Total parameters: {total_params:,}')\n",
        "print(f'   Trainable: {trainable_params:,} ({100 * trainable_params / total_params:.1f}%)')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Setup Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "print('\\n‚öôÔ∏è  Setting up trainer...')\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./checkpoints/{primary_dataset}_{EXPERIMENT_ID}\",\n",
        "    num_train_epochs=CONFIG['num_epochs'],\n",
        "    per_device_train_batch_size=CONFIG['batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    warmup_steps=CONFIG['warmup_steps'],\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,\n",
        "    report_to='none',\n",
        "    disable_tqdm=False,\n",
        ")\n",
        "\n",
        "# Setup compute_metrics function\n",
        "if task_config['task_type'] == 'ner':\n",
        "    def compute_metrics_fn(eval_pred):\n",
        "        return compute_ner_metrics(eval_pred, task_config['labels'])\n",
        "elif task_config['task_type'] in ['re', 'classification', 'qa']:\n",
        "    def compute_metrics_fn(eval_pred):\n",
        "        return compute_classification_metrics(eval_pred)\n",
        "elif task_config['task_type'] == 'multilabel_classification':\n",
        "    def compute_metrics_fn(eval_pred):\n",
        "        return compute_multilabel_metrics(eval_pred)\n",
        "elif task_config['task_type'] == 'similarity':\n",
        "    def compute_metrics_fn(eval_pred):\n",
        "        return compute_regression_metrics(eval_pred)\n",
        "\n",
        "# Create trainer\n",
        "callbacks = []\n",
        "if CONFIG['use_early_stopping']:\n",
        "    callbacks.append(EarlyStoppingCallback(early_stopping_patience=CONFIG['early_stopping_patience']))\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics_fn,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "print('   ‚úÖ Trainer ready')\n",
        "print(f'   Output: {training_args.output_dir}')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Train Model\n",
        "### üöÄ This is where the actual training happens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '='*60)\n",
        "print('üöÄ STARTING TRAINING')\n",
        "print('='*60)\n",
        "\n",
        "# Train!\n",
        "train_result = trainer.train()\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('‚úÖ TRAINING COMPLETE')\n",
        "print('='*60)\n",
        "print(f\"Train loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"Train time: {train_result.metrics['train_runtime']:.1f}s\")\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11: Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nüìä Evaluating on validation set...')\n",
        "\n",
        "# Evaluate\n",
        "eval_result = trainer.evaluate()\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('üìä EVALUATION RESULTS')\n",
        "print('='*60)\n",
        "for key, value in eval_result.items():\n",
        "    if 'f1' in key.lower() or 'precision' in key.lower() or 'recall' in key.lower():\n",
        "        print(f'{key}: {value:.4f}')\n",
        "print('='*60)\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'experiment_id': EXPERIMENT_ID,\n",
        "    'model': model_name,\n",
        "    'dataset': primary_dataset,\n",
        "    'task_type': task_config['task_type'],\n",
        "    'smoke_test': SMOKE_TEST,\n",
        "    'config': CONFIG,\n",
        "    'train_metrics': train_result.metrics,\n",
        "    'eval_metrics': eval_result,\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "results_file = RESULTS_DIR / f'results_{EXPERIMENT_ID}.json'\n",
        "with open(results_file, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f'\\n‚úÖ Results saved to: {results_file}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 12: Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '='*60)\n",
        "print('üéâ EXPERIMENT COMPLETE')\n",
        "print('='*60)\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Dataset: {primary_dataset}\")\n",
        "print(f\"Mode: {'SMOKE TEST' if SMOKE_TEST else 'FULL TRAINING'}\")\n",
        "print(f\"\\nF1 Score: {eval_result.get('eval_f1', 0):.4f}\")\n",
        "\n",
        "if SMOKE_TEST:\n",
        "    if eval_result.get('eval_f1', 0) > 0.30:\n",
        "        print('\\n‚úÖ Smoke test PASSED!')\n",
        "        print('   ‚Üí Set SMOKE_TEST = False for full training')\n",
        "    else:\n",
        "        print('\\n‚ùå Smoke test FAILED')\n",
        "        print('   ‚Üí Check configuration and data')\n",
        "else:\n",
        "    expected_f1 = 0.84 if primary_dataset == 'bc2gm' else 0.70\n",
        "    if eval_result.get('eval_f1', 0) > expected_f1 - 0.05:\n",
        "        print(f'\\n‚úÖ Result matches expected F1 (~{expected_f1:.2f})')\n",
        "    else:\n",
        "        print(f'\\n‚ö†Ô∏è  F1 lower than expected (~{expected_f1:.2f})')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('Next Steps:')\n",
        "if SMOKE_TEST:\n",
        "    print('1. Set SMOKE_TEST = False in Cell 4')\n",
        "    print('2. Run All Cells for full training')\n",
        "else:\n",
        "    print('1. Try different models (change model_name in Cell 3)')\n",
        "    print('2. Try different tasks (change datasets in Cell 3)')\n",
        "    print('3. Check results/ folder for saved metrics')\n",
        "print('='*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}